{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311f5da1-aacb-4e5c-b2fd-f24f85aa714c",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698dfaf-f017-41da-863d-d71ef23737ed",
   "metadata": {},
   "source": [
    "The IONOS AI Model Hub offers an OpenAI-compatible API that enables powerful text generation capabilities through foundation models. These Large Language Models (LLMs) can perform a wide variety of tasks, such as generating conversational responses, summaries, and contextual answers, without requiring you to manage hardware or extensive infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d536eeb-687a-490a-b1b3-029b1b5565ba",
   "metadata": {},
   "source": [
    "## Supported Text Generation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c35542-8204-4be0-b29f-beaef4a6163b",
   "metadata": {},
   "source": [
    "The following models are currently available for text generation, each suited to different applications:\n",
    "\n",
    "| Model Provider | Model Name | Purpose |\n",
    "|-----------------------------------------|---------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| openGPT-X ([<mark style=\"color:blue;\">**License**</mark>](https://www.apache.org/licenses/LICENSE-2.0.html)) | Teuken Commercial v0.4 Instruct (7B) | Ideal for dialogue use cases and natural language tasks in all 24 EU languages.| \n",
    "| Meta ([<mark style=\"color:blue;\">**License**</mark>](https://llama.meta.com/llama3_3/license/)) | Llama 3.3 Instruct (70B) | Ideal for dialogue use cases and natural language tasks: conversational agents, virtual assistants, and chatbots. Exceeds the capabilities of Llama 3.1 Instruct in quality (Llama 3.1 70B) and performance (Llama 3.1 405B).| \n",
    "| Meta ([<mark style=\"color:blue;\">**License**</mark>](https://llama.meta.com/llama3/license/)) | Llama 3.1 Instruct (8B, 70B and 405B) | Ideal for dialogue use cases and natural language tasks: conversational agents, virtual assistants, and chatbots. | \n",
    "| Meta ([<mark style=\"color:blue;\">**License**</mark>](https://ai.meta.com/llama/license/)) | Code Llama Instruct HF (13B) | Focuses on generating different kinds of computer code and understands programming languages. |\n",
    "| Mistral AI ([<mark style=\"color:blue;\">**License**</mark>](https://www.apache.org/licenses/LICENSE-2.0.html)) | Mistral Instruct v0.3 (7B), Mixtral (8x7B) | Ideal for conversational agents, virtual assistants, and chatbots; Comparison to Llama 3: better with European languages; supports longer context length. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30ec66-6c62-4183-92b9-467215c120bb",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb452024-168f-4444-aa11-6de1b1589625",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn how to generate text using foundation models via the IONOS API. This tutorial is intended for developers with basic knowledge of:\n",
    "- REST APIs\n",
    "- A programming language for handling REST API endpoints (Python and Bash examples are provided)\n",
    "\n",
    "By the end, you will be able to:\n",
    "1. Retrieve a list of text generation models available in the IONOS AI Model Hub.\n",
    "2. Apply prompts to these models to generate text responses, supporting applications like virtual assistants and content creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f05cc-cb7e-49b3-bc93-619abbe6832f",
   "metadata": {},
   "source": [
    "## Getting Started with Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c6255-e233-4e29-b9d9-caf63ba1d287",
   "metadata": {},
   "source": [
    "To use text generation models, first set up your environment and authenticate using the OpenAI-compatible API endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09e2f9-81ee-4a12-af74-f7ba2ff499b2",
   "metadata": {},
   "source": [
    "### Prerequisite: Access API Token from environment variable\n",
    "\n",
    "We strongly suggest that you save your IONOS API token as environment variable in your operating system. You can then access it using the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0bed00-d051-44d7-93cd-560eb939c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "IONOS_API_TOKEN = os.getenv('IONOS_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c78d97-9925-40de-b698-89caf2579885",
   "metadata": {},
   "source": [
    "### Step 1: Retrieve Available Models\n",
    "\n",
    "Fetch a list of models to see which are available for your use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8d0778-39d9-4cb8-99c0-906f83be67f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'black-forest-labs/FLUX.1-schnell',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'mistralai/Mistral-Small-24B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'BAAI/bge-m3',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'meta-llama/Llama-3.3-70B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'meta-llama/CodeLlama-13b-Instruct-hf',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'BAAI/bge-large-en-v1.5',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'mistralai/Mistral-7B-Instruct-v0.3',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'stabilityai/stable-diffusion-xl-base-1.0',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'mistralai/Mistral-Nemo-Instruct-2407',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'openGPT-X/Teuken-7B-instruct-commercial',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'meta-llama/Meta-Llama-3.1-405B-Instruct-FP8',\n",
       "   'object': 'model',\n",
       "   'created': 1677610602,\n",
       "   'owned_by': 'openai'}],\n",
       " 'object': 'list'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "endpoint = \"https://openai.inference.de-txl.ionos.com/v1/models\"\n",
    "\n",
    "header = {\n",
    "    \"Authorization\": f\"Bearer {IONOS_API_TOKEN}\", \n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "requests.get(endpoint, headers=header).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e656f-b820-4197-a770-cea7bcf3d702",
   "metadata": {},
   "source": [
    "This query returns a JSON document listing each models name, which youâ€™ll use to specify a model for text generation in later steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755395f9-6cd1-4746-b435-7219d1e57e24",
   "metadata": {},
   "source": [
    "### Step 2: Generate Text with Your Prompt\n",
    "\n",
    "To generate text, send a prompt to the chat/completions endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bfbccdb-33e2-42e6-9974-546b7e4a4a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-f9b59a8fc0ba4fe997a63ae8e7366732',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'message': {'content': \"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       "    'role': 'assistant',\n",
       "    'tool_calls': None,\n",
       "    'function_call': None}}],\n",
       " 'created': 1754233232,\n",
       " 'model': 'meta-llama/Llama-3.3-70B-Instruct',\n",
       " 'object': 'chat.completion',\n",
       " 'system_fingerprint': None,\n",
       " 'usage': {'completion_tokens': 25,\n",
       "  'prompt_tokens': 43,\n",
       "  'total_tokens': 68,\n",
       "  'prompt_tokens_details': None},\n",
       " 'service_tier': None,\n",
       " 'prompt_logprobs': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "MODEL_NAME = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "PROMPT = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    "\n",
    "endpoint = \"https://openai.inference.de-txl.ionos.com/v1/chat/completions\"\n",
    "\n",
    "header = {\n",
    "    \"Authorization\": f\"Bearer {IONOS_API_TOKEN}\", \n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "body = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"messages\": PROMPT,\n",
    "}\n",
    "requests.post(endpoint, json=body, headers=header).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aec5b6-d4b6-4597-ac7f-7d0f29a4b27a",
   "metadata": {},
   "source": [
    "### Step 3: Extract and Interpret the Result\n",
    "\n",
    "The returned JSON includes several key fields, most importantly:\n",
    "- **`choices.[].message.content`**: The generated text based on your prompt.\n",
    "- **`usage.prompt_tokens`**: Token count for the input prompt.\n",
    "- **`usage.completion_tokens`**: Token count for the generated output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f75c8-9305-4e2e-9d65-1e7bebb79f9d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "1. Access available text generation models.\n",
    "2. Use prompts to generate text responses, ideal for applications such as conversational agents, content creation, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merkle-tree-notebooks",
   "language": "python",
   "name": "merkle-tree-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
