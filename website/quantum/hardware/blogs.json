[
  {
    "title": "Tutorial 1 - The qubit or the two level system",
    "content": "\nIn this first tutorial we are going to discuss the two-level system as it is the simplest unit of quantum computing systems. We discuss on a physics level with Hamiltonians etc its static properties like level splitting and avoided crossings. Then we discuss dynamical processes like Rabi oscillations and their connection to the notation of quantum computation.\n\nThis lays the basis for the discussion of quantum computation hardware in the next tutorials. Namely:\n\n- superconducting qubits\n- trapped ions\n- neutral atoms\n\n## Hamiltonian, Eigenstates and Matrix Notation\n\nTo start out, we will consider two eigenstates $|0\\rangle,~|1\\rangle $ of the Hamiltonian $\\hat{H}_0$ with\n\n$$\n \\hat{H}_0|0\\rangle=E_0|0\\rangle, \\qquad \\hat{H}_0|1\\rangle=E_1|1\\rangle.\n$$\n\nQuite typically we might think of it as a two-level atom with states 0 and 1. The eigenstates can be expressed in matrix notation:\n\n$$\n |0\\rangle=\\left( \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right), \\qquad |1\\rangle=\\left( \\begin{array}{c} 0 \\\\ 1 \\end{array} \\right),\n$$\n\nso that $\\hat{H}_0$ be written as a diagonal matrix\n\n$$\n    \\hat{H}_0 = \\left(\\begin{array}{cc} E_0 & 0 \\\\ 0 & E_1 \\end{array}\\right).\n$$\n\nIf we would only prepare eigenstates the system would be rather boring. However, we typically have the ability to change the Hamiltonian by switching on and off laser or microwave fields. We can then write the Hamiltonian in its most general form as:\n\n$$\n\\hat{H} = \\frac{\\hbar}{2}\\left( \\begin{array}{cc} \\Delta  & \\Omega_x - i\\Omega_y\\\\ \\Omega_x +i\\Omega_y & -\\Delta \\end{array} \\right)\n$$\n\nSometimes we will also chose the definition:\n\n$$\n\\Omega = |\\Omega| e^{i\\varphi}=\\Omega_x + i\\Omega_y\n$$\n\nIt is particularly useful for the case in which the coupling is created by a laser. Another useful way of thinking about the two-level system is as a spin in a magnetic field. Let us remind us of the definitions of the of the spin-1/2 matrices:\n\n$$\ns_x = \\frac{\\hbar}{2}\\left(\\begin{array}{cc}\n0 & 1\\\\\n1 &  0\n\\end{array}\n\\right)~\ns_y = \\frac{\\hbar}{2}\\left(\\begin{array}{cc}\n0 & -i\\\\\ni &  0\n\\end{array}\n\\right)~s_z =\\frac{\\hbar}{2} \\left(\\begin{array}{cc}\n1 & 0\\\\\n0 &  -1\n\\end{array}\n\\right)\n$$\n\nWe then obtain:\n\n$$\n%\\label{Eq:HamSpin}\n\\hat{H} = \\mathbf{B}\\cdot\\hat{\\mathbf{s}}\\text{ with }\\mathbf{B} = (\\Omega_x, \\Omega_y, \\Delta)\n$$\n\nYou will go through this calculation in the excercise of this week.\n\n## Case of no perturbation $\\Omega = 0$\n\nThis is exactly the case of no applied laser fields that we discussed previously. We simply removed the energy offset $E_m = \\frac{E_0+E_1}{2}$ and pulled out the factor $\\hbar$, such that $\\Delta$ measures a frequency. So we have:\n\n$$\nE_0 = E_m+ \\frac{\\hbar}{2}\\Delta\\\\\nE_1 = E_m- \\frac{\\hbar}{2}\\Delta\n$$\n\nWe typically call $\\Delta$ the energy difference between the levels or the _detuning_.\n\n## Case of no detuning $\\Delta = 0$\n\nLet us suppose that the diagonal elements are exactly zero. And for simplicity we will also keep $\\Omega_y =0$ as it simply complicates the calculations without adding much to the discussion at this stage. The Hamiltonian reads then:\n\n$$\n\\hat{H} = \\frac{\\hbar}{2}\\left( \\begin{array}{cc} 0  & \\Omega\\\\ \\Omega &0 \\end{array} \\right)\n$$\n\nQuite clearly the states $\\varphi_{1,2}$ are not the eigenstates of the system anymore. How should the system be described now ? We can once again diagonalize the system and write\n\n$$\n\\hat{H}|\\varphi_{\\pm}\\rangle = E_{\\pm}|\\varphi_{\\pm}\\rangle\\\\\nE_{\\pm} = \\pm\\frac{\\hbar}{2}\\Omega\\\\\n|\\varphi_{\\pm}\\rangle = \\frac{|0\\rangle\\pm|1\\rangle}{\\sqrt{2}}\n$$\n\nTwo important consequences can be understood from this result:\n\n- The coupling of the two states shifts their energy by $\\Omega$. This is the idea of level repulsion.\n- The coupled states are a superposition of the initial states.\n\nThis is also a motivation the formulation of the 'bare' system for $\\Omega = 0$ and the 'dressed' states for the coupled system.\n\n## General case\n\nQuite importantly we can solve the system completely even in the general case. By diagonalizing the Hamiltonian we obtain:\n\n$$\n E_\\pm = \\pm \\frac{\\hbar}{2} \\sqrt{\\Delta^2+|\\Omega|^2}\n$$\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndeltaMax = 5\ndelta = np.linspace(-deltaMax, deltaMax, 100)\nomega = 1\n\nEplus = np.sqrt(delta**2+omega**2)/2\nEminus = -np.sqrt(delta**2+omega**2)/2\n\nf, ax = plt.subplots()\nax.plot(delta, Eplus, label=\"$E_+$\")\nax.plot(delta, Eminus, label=\"$E_+$\")\nax.legend()\nax.set_xlabel(\"detuning $\\Delta$\")\nax.set_ylabel(\"energy $E$\");\n```\n\n![png](quantum_hardware_101_files/quantum_hardware_101_3_0.png)\n\nThe Eigenstates then read:\n\n$$\n|\\psi_+\\rangle =\\cos\\left(\\frac{\\theta}{2}\\right) e^{-i{\\varphi}/{2}}|0\\rangle+\\sin\\left(\\frac{\\theta}{2}\\right) e^{i{\\varphi}/{2}}|1\\rangle\n$$\n\n$$\n|\\psi_-\\rangle =-\\sin\\left(\\frac{\\theta}{2}\\right)e^{-i{\\varphi}/{2}}|0\\rangle+\\cos\\left(\\frac{\\theta}{2}\\right) e^{i{\\varphi}/{2}}|1\\rangle\n$$\n\nwhere\n\n$$\n\\tan(\\theta) = \\frac{|\\Omega|}{\\Delta}\n$$\n\n## Dynamical Aspects\n\nAfter the static case we now want to investigate the dynamical properties of the two-state system. We calculate the time evolution of $|\\psi(t)\\rangle = c_0(t)|0\\rangle + c_1(t)|1\\rangle$ with the SchrÃ¶dinger equation and the perturbed Hamiltonian :\n\n$$\ni\\hbar \\frac{d}{dt}|\\psi(t)\\rangle=\\hat{H}|\\psi(t)\\rangle,\n$$\n\n$$\ni \\frac{d}{dt}\\left(\\begin{array}{c} c_0(t) \\\\ c_1(t) \\end{array}\\right) = \\frac{1}{2}\\left( \\begin{array}{cc} \\Delta & \\Omega \\\\ \\Omega^* & -\\Delta \\end{array} \\right) \\left(\\begin{array}{c} c_0(t) \\\\ c_1(t) \\end{array} \\right).\n$$\n\nWe have two coupled differential equations and we luckily already know how to solve them as we have calculated the two eigenenergies in the previous section. For the state $|\\psi(t)\\rangle$ we get\n\n$$\n |\\psi(t)\\rangle=\\lambda e^{-i{E_+}t/{\\hbar}} |\\psi_+\\rangle + \\mu e^{-i{E_-}t/{\\hbar}} |\\psi_-\\rangle\n$$\n\nwith the factors $\\lambda$ and $\\mu$, which are defined by the initial state. The most common question is then what happens to the system if we start out in the bare state $|0\\rangle$ and then let it evolve under coupling with a laser ? So what is the probability to find it in the other state $|1\\rangle$:\n\n$$\nP_1(t)=\\left|\\langle 1|\\psi(t)\\rangle\\right|^2.\n$$\n\nAs a first step, we have to apply the initial condition and express $|\\psi(0)\\rangle$ in terms of $|\\psi_{\\pm}\\rangle$:\n\n$$\n|\\psi(0)\\rangle \\overset{!}{=} |0\\rangle\n$$\n\n$$\n  =  e^{i{\\varphi}/{2}} \\left[ \\cos\\left( \\frac{\\theta}{2}\\right) |\\psi_{+}\\rangle-\\sin\\left(\\frac{\\theta}{2}\\right)|\\psi_{-}\\rangle\\right]\n$$\n\nBy equating the coefficients we get for $\\lambda$ and $\\mu$:\n\n$$\n\\lambda = e^{i{\\varphi}/{2}}\\cos\\left(\\frac{\\theta}{2}\\right), \\qquad  \\mu = -e^{i{\\varphi}/{2}}\\sin\\left(\\frac{\\theta}{2}\\right).\n$$\n\nOne thus gets:\n\n$$\n P_1(t)= \\left|e^{i\\varphi} \\sin\\left(\\frac{\\theta}{2}\\right)\\cos\\left(\\frac{\\theta}{2}\\right)\\left[e^{-i{E_+}t/{\\hbar}} - e^{-i{E_-}t/{\\hbar}}\\right]\\right|^2\n$$\n\n$$\n= \\sin^2(\\theta)\\sin^2\\left(\\frac{E_+-E_-}{2\\hbar}t\\right)\n$$\n\n$P_1(t)$ can be expressed with $\\Delta$ and $\\Omega$ alone. The obtained relation is called Rabi's formula:\n\n$$\n P_1(t)=\\frac{1}{1+\\left(\\frac{\\Delta}{|\\Omega|}\\right)^2}\\sin^2\\left(\\sqrt{|\\Omega|^2+\\Delta^2}\\frac{t}{2}\\right)\n$$\n\n```python\ndef rabi_osc(time: float, omega: float, delta: float) -> float:\n  \"\"\"\n  time evolution in the Rabi oscillation\n\n  Args:\n    time: time at which we measure\n    omega: coupling strength\n    delta: detuning\n\n  Returns:\n    float: probability to be in the excited state\n  \"\"\"\n  return 1/(1+(delta/omega)**2)*np.sin(np.sqrt(omega**2+delta**2)*time/2)**2\n\n\nomega = 2*np.pi*1\ntime = np.linspace(0,2, 100)\n\n\n\ndelta = 0\n\nf, ax = plt.subplots()\nax.plot(time, rabi_osc(time, omega, 0), label = \"$\\Delta = 0$\")\nax.plot(time, rabi_osc(time, omega, omega), label = \"$\\Delta = \\Omega$\")\nax.plot(time, rabi_osc(time, omega, 10*omega), label = \"$\\Delta = 10\\cdot\\Omega$\")\nax.legend()\nax.set_xlabel(\"time $t$\")\nax.set_ylabel(\"probability $P_1$\")\n```\n\n    Text(0, 0.5, 'probability $P_1$')\n\n![png](quantum_hardware_101_files/quantum_hardware_101_6_1.png)\n\nA few key words concerning Rabi oscillations are in order:\n\n- The probability to be in the excited state is indeed maximal if there is zero detuning.\n- The speed of the oscillations get higher with higher detuning. This fact is often overlooked at first sight but key in approximations like the _rotating wave approximation_.\n\n## A few words on the quantum information notation\n\nThe qubit is THE basic ingredient of quantum computers. However, you will typically not find Pauli matrices and other common notations of two-state systems in the platforms. The typical notation there is:\n\n- $R_x(\\phi)$ is a rotation around the x-axis for an angle $\\phi$.\n- Same holds for $R_y$ and $R_z$.\n- $X$ denotes the rotation around the x axis for an angle $\\pi$. So it transforms $|1\\rangle$ into $|0\\rangle$ and vise versa.\n- $Z$ denotes the rotation around the x axis for an angle $\\pi$. So it transforms $|+\\rangle$ into $|-\\rangle$ and vise versa.\n\nThe most commonly used gate is actually one that we did not talk about at all, it is the Hadamard gate, which transforms $|1\\rangle$ into $|-\\rangle$ and $|0\\rangle$ into $|+\\rangle$:\n\n$$\n\\hat{H}|1\\rangle = |-\\rangle\n$$\n\n$$\n\\hat{H}|0\\rangle = |+\\rangle\n$$\n\n$$\n\\hat{H}|-\\rangle = |1\\rangle\n$$\n\n$$\n\\hat{H}|+\\rangle = |0\\rangle\n$$\n\nIn the next tutorial we will see how these concepts are implemented in real hardware.\n\n```python\n\n```",
    "order": 1
  },
  {
    "title": "Tutorial 2 - Continuous variables or the quantum harmonic oscillator",
    "content": "\nIn the [first tutorial](./0) we saw a simple description of the qubit, which is quite often as far as hardware discussions go. Qubits are pretty awesome for the solution to a substantial amount of binary problems. However, the translation of continouous variables (CV) onto qubit is most of the time a rather sad moment. But to simply solve a problem with CV on a quantum system, there is no fundamental reason in going through a qubit representation.\n\nWe will examplify this at the widely discussed problem of the harmonic oscillator. This also lays the groundwork for the discussion of such hardware systems like photonic quantum systems, cold atoms or even the readout of qubit based hardware like trapped ions or superconducting devices.\n\n## The classical harmonic oscillator\n\nWithin physics we love to describe the evolution of a system with differential equations up to second order. The absolute classic is a particle of mass $m$ that is moving in a harmonic potential. We describe its position as $x$. Its equation of motion is then:\n\n$$\n\\ddot{x} +\\omega^2 x = 0\n$$\n\nThe solutions are then just oscillatory solutions:\n\n$$\nx(t) = a \\cos(\\omega t)+ b \\sin(\\omega t)\n$$\n\n$$\n\\dot{x}(t) = -a\\omega \\sin(\\omega t)+ b\\omega \\cos(\\omega t)\n$$\n\n$$\n\\ddot{x}(t) = -\\omega^2 x(t)\n$$\n\nThe two prefactors are are fully determined by:\n\n- The initial speed of the particle $\\dot{x}(t=0) = v_0$, or its associated momentum $p = mv$.\n- The initial position of the particle $x(t=0) = x_0$.\n\nThis results in:\n\n$$\nx(t=0) = a  = x_0\n$$\n\n$$\n\\dot{x}(t=0) = b\\omega = v_0\n$$\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy import pi\n\n```\n\n```python\n# steepness of the potential\nm = 1;\nomega = 2*np.pi*1;\n\n# initial conditions\nx0 = 0;\nv0 = 1;\n\ntlin = np.linspace(0,3,100);\n\nxt = x0*np.cos(omega*tlin)+v0/omega*np.sin(omega*tlin);\nvt = -x0*omega*np.sin(omega*tlin)+v0*np.cos(omega*tlin);\n\nf, (ax1,ax2) = plt.subplots(2,1, sharex=True)\nax1.plot(tlin, xt)\nax2.plot(tlin, vt)\nax1.set_xlabel('time $t$')\nax1.set_ylabel('position $x$')\nax2.set_ylabel('velocity $v$')\n```\n\n    Text(0, 0.5, 'velocity $v$')\n\n![png](quantum_hardware_102_files/quantum_hardware_102_3_1.png)\n\nWe learn a few things from this example:\n\n- The position and momentum are independent variables of the problem.\n- If we cannot solve the equations of motion analytically or numerically, we have to resort to an _'experiment'_ that solves the differential equation for us. However, every _'experiment'_ will only solve one eom for one set of parameters for us.\n\n### Random initial conditions\n\nWe have seen that the system might be solved analytically, however this might hit some problems for more complex problems. Then we have to investigate the solutions experimentally. For this work, we will prepare the system with some fidelity in some initial state. We assume:\n\n- We have some precision $\\Delta x$ on the initial state preparation at $x_0$.\n- We have some precision $\\Delta v$ on the initial state preparation at $v_0$.\n\nLet us just assume a gaussian for simplicity. To describe this initial state we can then use the probability distribution $P_0(x)$ and $P_0(v)$. For a finite grid size it means that we describe the initial state by a vector whose a elements are positive real numbers between 0 and 1 with the normalization condition:\n\n$$\n\\sum p_i = 1\n$$\n\n```python\nv0 = 1;dv = 0.1;\nx0 = 0; dx = 0.1;\n\nPx = np.random.normal(x0, dx, 500)\nPv = np.random.normal(v0, dv, 500)\n\nf, (ax1,ax2) = plt.subplots(2,1)\nax1.hist(Px, density=True)\nax2.hist(Pv, density=True)\nax1.set_xlabel('initial position $x$')\nax2.set_xlabel('initial velocity $v$')\nf.tight_layout()\n```\n\n![png](quantum_hardware_102_files/quantum_hardware_102_6_0.png)\n\n\\And depending on these initial conditions, we will obtain some final distributions $P(x_f)$ and $P(v_f)$\n\n```python\n# initial conditions\nv0 = 3;dv = 0.1;\nx0 = 0; dx = 0.1;\nNtries = 100;\n\nx0s = np.random.normal(x0, dx, Ntries);\nv0s = np.random.normal(v0, dv, Ntries);\n\n\ntlin = np.linspace(0,3,100);\n\nxts = np.zeros((100, Ntries));\nvts = np.zeros((100, Ntries));\nfor ii in np.arange(Ntries):\n    xts[:,ii] = x0s[ii]*np.cos(omega*tlin)+v0s[ii]/omega*np.sin(omega*tlin);\n    vts[:,ii] = -x0s[ii]*omega*np.sin(omega*tlin)+v0s[ii]*np.cos(omega*tlin);\n\nf, (ax1,ax2) = plt.subplots(2,1, sharex=True)\nax1.plot(tlin, xts, 'bo', alpha = 0.1)\nax2.plot(tlin, vts, 'bo', alpha = 0.1)\nax1.set_xlabel('time $t$')\nax1.set_ylabel('position $x$')\nax2.set_ylabel('velocity $v$')\n```\n\n    Text(0, 0.5, 'velocity $v$')\n\n![png](quantum_hardware_102_files/quantum_hardware_102_8_1.png)\n\nquite importantly we can only test one set of initial conditions for each trajectory. The particle cannot be at several positions at the same time. With quantum mechanics, we can now look into a system where this restriction gets lifted.\n\n## Quantum harmonic oscillator\n\n### Finding the Hamiltonian\n\nTo find the quantum mechanical analog to some classical equations of motion, we must re-express them in terms of a **Lagrangien** $L$. It is a functional, which depends on the position $x$ and the independent velocity $\\dot{x}$. The equations of motion of the particle are then given as:\n\n$$\n\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{x}}\\right) = \\frac{\\partial L}{\\partial x}\n$$\n\nFor the harmonic oscillator we have the Lagrangien:\n\n$$\nL_{h.o.} = \\frac{m\\dot{x}^2}{2}-\\frac{m\\omega^2}{2}x^2\n$$\n\nWe can then identify from it several fundamental quantities:\n\n- the kinetic energy of the particle $T = \\frac{m\\dot{x}^2}{2}$.\n- Its potential energy $V = \\frac{m\\omega^2}{2}x^2$.\n- Its total energy $E = T + V$\n\nHowever, for the translation to quantum mechanics it is even more important that the Lagrangien allows us to define the _conjugate momentum_ to the position:\n\n$$\np = \\frac{\\partial L}{\\partial \\dot{x}}\n$$\n\nWe then obtain the classical Hamiltonian for the system, which is:\n\n$$\nH = \\dot{x} p - L\n$$\n\n$$\nH_{h.o.} = \\frac{p^2}{2m}+\\frac{m\\omega^2}{2}x^2\n$$\n\nWe have now assembled all ingridients to 'quantize' the system.\n\n- We know the Hamiltonian.\n- We know its conjugate variables.\n\nThe system is then described by the Hamiltonian:\n\n$$\n\\hat{H}_{H.O.} = \\frac{\\hat{p}^2}{2m} + \\frac{m\\omega^2}{2}\\hat{x}^2\n$$\n\nand the commutation relationship:\n\n$$\n[\\hat{x}, \\hat{p}] = i\\hbar\n$$\n\n$$\n[\\hat{x}, \\hat{p}] \\equiv \\hat{x} \\hat{p} - \\hat{p}\\hat{x}\n$$\n\nThe fact that the commutators are not disappearing is telling us that for any quantum system we cannot observe its position and momentum to arbitrary position at the same time. A few notes before we continue:\n\n- In the quantum mechanical system we can still have completely continouous observables $\\hat{x}$ and $\\hat{p}$. Nothing has changed here.\n- As we will see we now have a quantized energy spectrum, which is a fundamental change to quantum mechanics.\n- As we will also see, we can launch the system in a superposition of different initial conditions, which are then explored in a parallel fashion.\n\n### Time evolution\n\nThis set of equations allows us to obtain the equations of motion through the Heisenberg equation as:\n\n$$\n\\frac{d \\hat{x}}{dt} = \\frac{i}{\\hbar}[\\hat{H}, \\hat{x}]\n$$\n\n$$\n\\frac{d \\hat{p}}{dt} = \\frac{i}{\\hbar}[\\hat{H}, \\hat{p}]\n$$\n\nFor the position the harmonic oscillator we obtain then:\n\n$$\n\\frac{d \\hat{x}}{dt} = \\frac{i}{\\hbar}[\\hat{H}_{H.O.}, \\hat{x}]\n$$\n\n$$\n= \\frac{i}{\\hbar}[\\frac{p^2}{2m}, \\hat{x}]\n$$\n\n$$\n= \\frac{\\hat{p}}{m}\n$$\n\nand for its momentum we obtain:\n\n$$\n\\frac{d \\hat{p}}{dt} = \\frac{i}{\\hbar}[\\hat{H}_{H.O.}, \\hat{p}]\n$$\n\n$$\n= \\frac{i}{\\hbar}[\\frac{m\\omega^2}{2} \\hat{x}^2, \\hat{p}]\n$$\n\n$$\n= -m\\omega^2 \\hat{x}\n$$\n\nSo we are basically back at the set of differential equations, which looks similiar to what we had previously, except of th funny operator symbol on top of $x$ and $p$. For all practical purposes you might see these operators as some massive matrices acting on some kind complex vectors $|\\psi\\rangle$. A few comments here:\n\n- To solve the problem you have to numerically solve the time evolution of the massive matrix and not only of one variable. One example of quantum paralellism.\n- The state vectors contain the initial state as we will see in a moment.\n- To solve the problem we typically decompose the matrices into the eigenstates and eigenvalues, which are then the energies.\n- If you want to solve the problem analytically it is most common to work with the SchrÃ¶dinger picture, but it looks a bit different on first sight and we will just leave it to the appendix for the moment.\n\n### Initial conditions\n\nIn the quantum mechanical system we do not describe the initial conditions through its probability distributions anymore, but rather the wavefunction $|\\psi_0\\rangle$.\n\n- Imagine that we are now observing the position of the particle. Each outcome is noted $|x_i \\rangle$. It is basically the points on the grid with a spacing set by the resolution of the detector or the size of my quantum computer etc. This discretization is just for a simpler discussion, you can also just replace the sums, by integrals if you would like.\n- To each outcome we associate some complex number $\\psi(x_i)$ to describe the initial condition.\n- We obtain the probability distribution $P(x_i) = |\\psi(x_i)|^2$\n\nWe then typically write the initial state as:\n\n$$\n|\\psi_{init}\\rangle = \\int dx~\\psi(x) |x\\rangle\\langle x|\n$$\n\nOr if you prefer the discrete version:\n\n$$\n|\\psi_{init}\\rangle = \\sum_i \\alpha_i|x_i\\rangle\n$$\n\nSo we can describe the initial state, by a vector whose entries are complex numbers and which fulfill the normalization conditions:\n\n$$\n\\int dx~ |\\psi(x)|^2 = 1\n$$\n\nFor conciseness it is always nice to just use a Gaussian with width $\\sigma_x$\n\n```python\nxlin = np.linspace(-10,10, 100);\nsigmax = 1;\n# initial shape\npsiInit = 1/(2*np.pi*sigmax**2)**(1/4)*np.exp(-xlin**2/4/sigmax**2);\n\n# normalization\nnp.trapz(psiInit**2, xlin)\n\nf, ax = plt.subplots()\nax.plot(xlin, psiInit)\nax.set_xlabel('position (x)')\nax.set_ylabel('$\\psi$')\n```\n\n    Text(0, 0.5, '$\\\\psi$')\n\n![png](quantum_hardware_102_files/quantum_hardware_102_15_1.png)\n\nBut what about the momentum distribution ? In the same way as we were able to assign some values to the initial distribution of the position, we can associate some distribution for the distribution in momentum:\n\n- We will note every possible outcome for position as $|k_j \\rangle$\n- We will associate its components with some complex value $\\beta_j$\n\nWe then typically write the initial state as:\n\n$$\n|\\psi_{init}\\rangle = \\int dk~\\psi(k) |k\\rangle\\langle k|\n$$\n\nOr if you prefer the discrete version:\n\n$$\n|\\psi_{init}\\rangle = \\sum_j \\beta_j|k_j\\rangle\n$$\n\nOnce again the entries are complex numbers and fulfill the normalization condition:\n\n$$\n\\int dk~ |\\psi(k)|^2 = 1\n$$\n\nThe two representations are not independent. They are connected through the Fourier transform:\n\n$$\n\\psi(k) = \\frac{1}{\\sqrt{2\\pi}}\\int dx \\psi(x) e^{-ikx}\n$$\n\nFor a Gaussian of width $\\sigma_x$ in position space, this results in a gaussian of width $\\sigma_k = 1/2\\sigma_x$\n\n```python\nxlin = np.linspace(-10,10, 100);\nklin = np.linspace(-10,10, 100);\nsigmax = 1;\nsigmak = 1/2/sigmax;\n# initial shape\npsiInit_x = 1/(2*np.pi*sigmax**2)**(1/4)*np.exp(-xlin**2/4/sigmax**2);\npsiInit_k = 1/(2*np.pi*sigmak**2)**(1/4)*np.exp(-klin**2/4/sigmak**2);\n\n# normalization\nnp.trapz(psiInit_x**2, xlin)\n\nf, (ax1,ax2) = plt.subplots(2,1)\nax1.plot(xlin, psiInit_x)\nax1.set_xlabel('position $x$')\nax1.set_ylabel('$\\psi(x)$')\nax2.plot(klin, psiInit_k)\nax2.set_xlabel('momentum $k$')\nax2.set_ylabel('$\\psi(k)$')\nf.tight_layout()\n```\n\n![png](quantum_hardware_102_files/quantum_hardware_102_17_0.png)\n\nIt is of fundamental importance to recognize that we can now choose initial conditions in which the particle is in several initial conditions at the same time. As an example it is perfectly valid to have a particle which is centered at zero, but is in a super position of opposite velocities.\n\n```python\nimport scipy.fftpack as ft\n\nNx = 256;Nk = 100;\nxlin = np.linspace(-10,10, Nx);\n\n# size of the wavepackage\nsigmax = 1;\n#initial velocity\nk0 = 4;\n\n# initial shape\npsiInit_x = 1/(2*np.pi*sigmax**2)**(1/4)*np.exp(-xlin**2/4/sigmax**2)*1/np.sqrt(2)*(np.exp(1j*k0*xlin)+np.exp(-1j*k0*xlin));\n\n# normalization\nNx = np.trapz(abs(psiInit_x)**2, xlin);\nprint(Nx)\n\n\n# get its Fourier transform\ndx = np.diff(xlin).mean()\nk_fft = ft.fftfreq(xlin.size,d = dx)\nk_fft = 2*np.pi*ft.fftshift(k_fft)\ndk = np.diff(k_fft).mean()\n\npsiInit_fft = np.fft.fftshift(np.fft.fft(psiInit_x));\n# normalization\nNk = np.trapz(abs(psiInit_fft)**2, k_fft);\npsiInit_fft = psiInit_fft/np.sqrt(Nk)\n\nf, (ax1,ax2) = plt.subplots(2,1)\nax1.plot(xlin, abs(psiInit_x)**2)\nax1.set_xlabel('position $x$')\nax1.set_ylabel('$\\psi(x)$')\n#ax2.plot(klin, psiInit_k)\nax2.plot(k_fft, abs(psiInit_fft)**2)\nax2.set_xlabel('momentum $k$')\nax2.set_ylabel('$\\psi(k)$')\nf.tight_layout()\n```\n\n    1.0000000000000124\n\n![png](quantum_hardware_102_files/quantum_hardware_102_19_1.png)\n\n## Connection the dots\n\nWe have now seen how to define the initial conditions and how obtain the time evolution of momentum and position. To connect the two of them, we\nhave to look into expectation values, which are defined as:\n\n$$\nx(t) = \\langle\\psi_0| \\hat{x}(t) |\\psi_0\\rangle\n$$\n\n$$\np(t) = \\langle\\psi_0| \\hat{p}(t) |\\psi_0\\rangle\n$$\n\nThis also allows us to obtain some typical classical equations of motion:\n\n$$\n\\langle\\psi_0|\\frac{d \\hat{x}}{dt} |\\psi_0\\rangle = \\frac{\\langle\\psi_0|\\hat{p}|\\psi_0\\rangle}{m}\n$$\n\nand for its momentum we obtain:\n\n$$\n\\langle\\psi_0|\\frac{d \\hat{p}}{dt} |\\psi_0\\rangle = -m\\omega^2 \\langle\\psi_0|\\hat{x}|\\psi_0\\rangle\n$$\n\n```python\nfrom numpy import linalg as LA\n\nNx = 256;\nxvec = np.linspace(-10,10, Nx);\n\nomega = 1; m = 1; hbar = 1;\n\n# kinetic energy\ndx = np.diff(xvec).mean()\n\ndia = -2*np.ones(Nx)\noffdia = np.ones(Nx-1)\nd2grid = np.mat(np.diag(dia,0) + np.diag(offdia,-1) + np.diag(offdia,1))/dx**2\nd2grid[0,:]=0\nd2grid[Nx-1,:]=0\n\nEkin = -hbar**2/(2*m)*d2grid\nEkin\n\n# potential energy\nx = np.mat(np.diag(xvec,0))\nVx = m*omega**2/2*xvec**2;\nEpot = np.mat(np.diag(Vx,0))\n\nH =  Ekin + Epot\n\n# diagonalization\nw, v = LA.eig(H)\n# sort it such that things look nice later\nsortinds = np.argsort(w)\nEigVecs = v[:,sortinds]\nEigVals = w[sortinds]\n\n# size of the wavepackage\nsigmax = 1;\n\n#initial velocity\nk0 = 4;\n\n# initial shape\npsiInit_x = 1/(2*np.pi*sigmax**2)**(1/4)*np.exp(-xlin**2/4/sigmax**2)*1/np.sqrt(2)*(np.exp(1j*k0*xlin)+np.exp(-1j*k0*xlin));\n#psiInit_x = 1/(2*np.pi*sigmax**2)**(1/4)*np.exp(-xlin**2/4/sigmax**2)*np.exp(1j*k0*xlin);\n#psiInit_x = 1/(2*np.pi*sigmax**2)**(1/4)*np.exp(-xlin**2/4/sigmax**2)*np.exp(-1j*k0*xlin);\npsiInit_x = np.mat(psiInit_x).T\n#transform to matrix\nEigVals = np.mat(EigVals).T\n\n# EigVecs contains the Eigenvectors as columns,\n# conjugate transpose times Psi0 calculates the scalar product\n# of every energy-eigenvector with Psi0 (yields the representation in the energy-eigenbasis)\nPsi0Eig = EigVecs.H * psiInit_x\n\ntmax = 30\nNSteps = 300\ndt = tmax/NSteps\nmeanx = np.zeros(NSteps)\nxsq = np.zeros(NSteps)\nts = np.zeros(NSteps)\npsi_t = 1j*np.zeros((Nx, NSteps))\nn_t = np.zeros((Nx, NSteps))\nt = 0\nfor k in range(0,NSteps):\n\n    # Time evolution is just a phase evolution in the energy eigenbasis\n    PsitEig = np.multiply( np.exp(-1j*EigVals/hbar*t),Psi0Eig )\n\n    # Transform back to the grid representation (calculate the time-evolved\n    # superposition of energy-eigenstates)\n    Psit = EigVecs*PsitEig\n    psi_t[:,k] = Psit.flatten();\n    n_t[:,k] = np.array(np.abs(Psit.flatten()))**2;\n    # calculate the mean position\n    meanx[k] = np.real(Psit.H*x*Psit);\n    xsq[k] = np.real(Psit.H*x**2*Psit);\n    t = t+dt\n    ts[k]=t\n\nxstd = np.sqrt(xsq-meanx**2);\n\nf, ax = plt.subplots()\nax.pcolormesh(ts, xvec, n_t);\n```\n\n![png](quantum_hardware_102_files/quantum_hardware_102_21_0.png)\n\n## So what is discrete or quanitized in the quantum harmonic oscillator ?\n\nWe have seen now that one fundamental difference between the classical and the quantum implementation of the harmonic oscillator is the possibility of superpositions in the initial state. This did not change that the observables are perfectly continuous variables. However the dramatic change happens in the accessible energy levels.\n\n### Energies for the classical harmonic oscillator\n\nFor the classical harmonic oscillator the energy is directly linked to the speed and the position of the particle and it is given as:\n\n$$\nE_{h.o.} = \\frac{mv^2}{2}+\\frac{m\\omega^2}{2}x^2\n$$\n\nSo it can take any value. For the quantum system this is dramatically different. The energies are quantized now and given by\n\n$$\nE_n = \\hbar \\omega(n+1/2)\\mbox{ with }n\\in \\mathbb{N}\n$$\n\nSo this thing is now discrete in the quantum version of the harmonic oscillator, not more and not less.\n\n## Summary\n\nIn this tutorial we have seen the description of the quantum harmonic oscillator, how it connects to certain problems and how to solve this numerically. In the [third tutorial](qhw3), we will see how it is used to trap and cool ions, which are one of the leading quantum computation platforms.\n\n```python\n\n```",
    "order": 2
  },
  {
    "title": "Tutorial 3 - A few words about quantum computing with trapped ions",
    "content": "\n\nIn this tutorial, we are going to discuss the fundamental ingredients for\nquantum computing with trapped ions.\n\n- In a first step, we discuss trapping and cooling in a harmonic oscillator, which was introduced in the [second tutorial](qhw2).\n- Then we can discuss the implementation and control of the single qubit as generally summarized in the [first tutorial](qhw1) operations.\n- Finally, we discuss how two ions are entangled through different two-qubit gates.\n\nEven if an enormous amount of additional literature\nexists, I will only reference here to a [nice introduction](https://arxiv.org/abs/1804.03719v1) and a more complete list is left for future discussions.\n\n## What do we want from a quantum computing hardware ?\n\nIn a QC we would like to implement algorithms, which are based on well\ndefined operations. Influential examples of such algorithms are the\nquantum Fourier transform and the Grover algorithm.\n\nGiven that computations are often implemented through logical truth\ntables, we typically base a quantum computer on qubits. We then call one\nstate $\\left|0\\right\\rangle$ and on\n$\\left|1\\right\\rangle$. Given that we would like to have\nreproducable computations, we always assume that we start them out with\nall qubits in the $\\left|0\\right\\rangle$ state.\n\nA computation then consists then in applying a number of gates. The key is\nhere that any algorithm might be built up from an extremely limited\nnumber of gates. Typically four are sufficient:\n\n- The three gates that rotate each individual qubit on the Bloch\n  sphere.\n\n- A gate that entangles them properly. The standard example is here\n  the CNOT gate, which we will come back too.\n\nSuch computations are then typically nicely visualized through circuit\ndiagrams often resulting from the interaction with programming framworks like `pennylane` or `qiskit`. To make the programs flexible enough, we might want a checklist of what we want from a quantum computer hardware. DiVincenzo proposed\nthe [following ingredients](https://www.sciencedirect.com/science/article/abs/pii/S0370157308003463?via%3Dihub):\n\n1.  Qubits that can store information in a scalable system.\n\n2.  The ability to initialize the system in the right state.\n\n3.  A universal set of gates.\n\n4.  Long coherence times, which are much longer than gate operation times.\n\n5.  Good measurement capabilities\n\nTrapped ions allow us to fulfill all these requirements as we will see\nin this lecture and we will go through them step-by-step.\n\n## Trapping and cooling\n\nFor computing experiments one typically works with singe-charged ions\nlike $Ca^+$. Given their charge, they can be trapped in very clean\ntraps under vacuum. As such they are extremely well isolated from the\nenvironment and high precision experiments can be performed. Finally,\nthey have only one remain electron in the outer shell. Therefore they\nhave a hydrogenlike atomic structure.\n\nHowever, the trap construction is not trivial given Maxwells equation\n$\\text{div} \\vec{E} = 0$. So, the experimentalists have to play some\ntricks with oscillating fields. We will not derive in detail how a\nresulting **Paul trap** works, but the [linked\nvideo](https://youtu.be/Xb-zpM0UOzk) gives a very nice impression of the\nidea behind it.\n\nThis work on trapping ions dates back to the middle of the last century\n(!!!) and was recognized by the[ Nobel prize in\n1989](https://www.nobelprize.org/prizes/physics/1989/summary/) for\nWolfgang Paul and Hans Dehmelt. They shared\nthe prize with Norman Ramsey, who developped extremely precise\nspectroscopic methods, now known as Ramsey spectroscopy.\n\nA Paul trap provides a harmonic oscillator confinement with trapping\nfrequencies in the order of hundreds of kHz. An ion trapped in such a\ntrap can the be described by the Hamiltonian:\n\n$$\n\\begin{aligned}\n\\hat{H}_{t} &= \\frac{\\hat{p}^2}{2m}+ \\frac{m\\omega_t^2}{2}\\hat{x}^2\\end{aligned}\n$$\n\nThe two variables $p$ and $x$ are non-commuting $[x, p] = i\\hbar$, so\nthey cannot be measured at the same time. It can be nicely diagonalized\nin terms of the ladder operators:\n\n$$\n\\begin{aligned}\n\\hat{x} &= \\sqrt{\\frac{\\hbar}{2m\\omega_t}}\\left(\\hat{a}+\\hat{a}^\\dagger\\right)\\\\\n\\hat{p} &= i\\sqrt{\\frac{\\hbar}{2m\\omega_t}}\\left(\\hat{a}^\\dagger-\\hat{a}\\right)\\\\\\end{aligned}\n$$\n\nSo the Hamiltonian can now be written as:\n\n$$\n\\begin{aligned}\n\\hat{H} &= \\hbar \\omega_t \\left(\\hat{N} + \\frac{1}{2}\\right)\\text{ with } \\hat{N} = a^\\dagger a\n\\end{aligned}\n$$\n\nHaving loaded the ions into the Paul trap we also need to cool them\ndown.\n\n## Atom-light interaction\n\nGiven that the ions keep only on atom on the outer shell, they have a\nhydrogenlike structure, which makes them well controllable with light.\n\nExperimentally, we will then use light of amplitude $E_0$ and frequency $\\omega_L$:\n\n$$\n\\begin{aligned}\n\\vec{E}(t) &= \\vec{E}_0 \\cos(kx - \\omega_L t+\\varphi)\\\\\n&= \\frac{\\vec{E}_0}{2} \\left(e^{i[kx - \\omega_lt+\\varphi]}+e^{-i[kx-\\omega_lt+\\varphi]}\\right) \\end{aligned}\n$$\n\nWe will describe the interal states of the ion for the moment with the\nsimple two state system of ground state\n$\\left|g\\right\\rangle$ and excited state\n$\\left|e\\right\\rangle$ at an energy $\\hbar \\omega_0$, which\nis typically in the order of thousands of THz. It has the Hamiltonian:\n\n$$\n\\begin{aligned}\nH_{ion} = \\hbar \\omega_0 \\left|e\\right\\rangle\\left\\langle e\\right|\\end{aligned}\n$$\n\nPutting this ion into propagating light will induce a coupling between\nthese two internal states. As previously , we will describe the coupling\nin the semi-classical approximation through\n$H_\\textrm{int} = -\\hat{\\vec{D}} \\cdot \\vec{E}$. However, in this\ncontext we will not ignore the propagating nature of the light field and\nkeep its position dependence. This is necessary as we would like to\nunderstand how the light influences the movement of the atoms and not\nonly the internal states. Putting them together we obtain:\n\n$$\n\\begin{aligned}\nH_\\textrm{int} &= \\frac{\\Omega}{2}\\left([\\left|g\\right\\rangle\\left\\langle e\\right|+\\left|e\\right\\rangle\\left\\langle g\\right|]e^{i(k \\hat{x} - \\omega_L t+\\varphi)} + h.c.\\right)\\end{aligned}\n$$\n\nThe laser frequency is tuned closely to the frequency of the internal\nstate transition and we will be only interested in the detuning\n$\\Delta = \\omega_0 - \\omega_L$. Importantly, it couples the position of\nthe atom and the internal states.\n\nTo simplify the problem, we can work in the rotating frame to describe\nthe external and internal degrees of freedom for the ion:\n\n$$\n\\begin{aligned}\n\\hat{H}= \\hbar \\omega_t \\hat{a}^\\dagger \\hat{a} + \\hbar\\Delta \\left|e\\right\\rangle\\left\\langle e\\right| + \\frac{\\Omega}{2}\\left(\\left|e\\right\\rangle\\left\\langle g\\right|e^{i\\left(k \\hat{x}+\\varphi\\right)} + h.c.\\right)\\end{aligned}\n$$\n\nWe will now see how this system is used to cool the ions to the motional\ngroundstate, perform single qubit operations and then two-qubit\noperations.\n\n## Doppler cooling\n\nThis interaction of the atom with a photon is at the origin of the\nall-important Laser cooling, which was pioneered for ions in the 1970s\n(!!) by the Wineland group. For cooling transition we couple the ground\nstate to an excited state of finitie lifetime $\\tau= \\frac{1}{\\Gamma}$.\n\nThis laser cooling had a tremendous impact on the field of atomic\nphysics in general. This importance was\nrecognized in the [Nobel prizes of 1997](https://www.nobelprize.org/prizes/physics/1997/summary/) for Steve Chu , Claude\nCohen-Tannoudji and Bill Phillips.\n\n### Working in the Lamb-Dicke regime\n\nAfter this initial cooling stage the atoms have to be cooled to the\nground state in the trap. To treat the trapped particles we will express\nthe position operator in terms of the ladder operator, such that:\n\n$$\n\\begin{aligned}\nk\\hat{x} &= \\eta (\\hat{a}^\\dagger+ \\hat{a})\\\\\n\\eta &= \\sqrt{\\frac{\\hbar^2 k^2/2m}{\\hbar \\omega_t}} =\\sqrt{\\frac{E_R}{\\hbar \\omega_t}}\\end{aligned}\n$$\n\n$\\eta$ is called the _Lamb-Dicke_ parameter. It compares the change in\nmotional energy due to the absorption of the photon\n$E_r = \\frac{(\\hbar k)^2}{2m}$ compared to the energy spacing\n$\\hbar \\omega_t$ in the trap. When it is small it suppresses the change\nof the motional state of the atom due to the absorption of a photon.\n\nFor simplicity we will set in this section $\\varphi=0$ and develop the\nexponent to obtain:\n\n$$\n\\begin{aligned}\nH_\\textrm{int} &= \\frac{\\Omega}{2}\\left(\\left|e\\right\\rangle\\left\\langle g\\right|\\left(1 + i\\eta[\\hat{a}^\\dagger+ \\hat{a}]\\right) + h.c.\\right)\\end{aligned}\n$$\n\nSo it contains three couplings for different trap levels and internal\nstates:\n\n- The _carrier_ transition\n  $\\left|g,n\\right\\rangle\\rightarrow \\left|e,n\\right\\rangle$\n  with strength $\\Omega$.\n\n- The _red_ sideband\n  $\\left|g,n\\right\\rangle\\rightarrow \\left|e,n-1\\right\\rangle$\n  with strength $\\eta \\Omega(n+1)$. It leads to a reduction of the\n  trap level and it is resonant for $\\Delta = -\\omega_t$.\n\n- The _blue_ sideband\n  $\\left|g,n\\right\\rangle\\rightarrow \\left|e,n+1\\right\\rangle$\n  with strength $\\eta \\Omega n$. It leads to an increase of the trap\n  level and it is resonant for $\\Delta = \\omega_t$.\n\nThis scheme is used to perform **Raman side-band cooling**. The laser is\ntuned on the transition\n$\\left|n,g\\right\\rangle\\rightarrow \\left|n-1,e\\right\\rangle$\nsuch that each absorption involves a reduction in the trap level. This\nset-up for cooling was first demonstrated in 1995 by the Wineland group.\n\nIt is at this stage that the ions are in the motional ground state and\nwe can focus our attention to the high control of the internal qubit\nstates of the ion for quantum computing.\n\n## Single-qubit operations\n\nThe single qubit operations can now be identified with the transition\n$\\left|e,n\\right\\rangle\\leftrightarrow \\left|g,n\\right\\rangle$.\nWe can then simplify the above equation too:\n\n$$\n\\begin{aligned}\n\\hat{H}= \\hbar\\Delta \\left|e\\right\\rangle\\left\\langle e\\right| + \\frac{\\hbar\\Omega}{2}\\left(\\left|e\\right\\rangle\\left\\langle g\\right|e^{i\\varphi} +\\left|g\\right\\rangle\\left\\langle e\\right|e^{-i\\varphi}\\right)\n\\end{aligned}\n$$\n\nWe can translate this into the language of qubit operations through the\ndefinitions:\n\n$$\n\\begin{aligned}\n\\sigma_z &= \\frac{\\left|e\\right\\rangle\\left\\langle e\\right|-\\left|g\\right\\rangle\\left\\langle g\\right|}{2}\\\\\n\\sigma_x &= \\frac{\\left|e\\right\\rangle\\left\\langle g\\right|+\\left|g\\right\\rangle\\left\\langle e\\right|}{2}\\\\\n\\sigma_y &= \\frac{i\\left|e\\right\\rangle\\left\\langle g\\right|-i\\left|g\\right\\rangle\\left\\langle e\\right|}{2}\n\\end{aligned}\n$$\n\nSo we can now simply write the Hamiltonian as:\n\n$$\n\\begin{aligned}\n\\hat{H}/\\hbar &= \\Delta \\sigma_z +\\Omega_x \\sigma_x +\\Omega_y \\sigma_y\\\\\n\\Omega_x &= \\Omega \\cos(\\varphi)\\\\\n\\Omega_y &= \\Omega \\sin(\\varphi)\n\\end{aligned}\n$$\n\nIn the QC community people rarely talk about the Pauli matrices, but\nmuch rather about a few specific gates. The most cited here is the\n_Hadamard_ gate, which transforms\n$\\left|0/1\\right\\rangle\\rightarrow \\frac{\\left|0\\right\\rangle\\pm\\left|1\\right\\rangle}{\\sqrt{2}}$.\nSo it has no good classical analog. Further a double application brings\nus back to the origin.\n\nThe other gate we named about was a Z gate, which is simply a $\\pi$\nrotation around the z axis.\n\n## Two-qubit operations\n\nTo implement a quantum computer the system has to be completed by a\ntwo-qubit operation. For ions a number of two-qubit gates exist:\n\n- The **Cirac-Zoller** gate was the [first proposed](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.74.4091) two-qubit gate and it was also the first one realized within the [same year](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.75.4714) .\n\n- The [**Soerensen-Moelmer**](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.82.1971) gate was proposed later, but it is extremely important from a practical point of view as it leads to very high entanglement fidelities.\n\n- Another realization, which we mention for completeness is the [geometric phase-gate](https://www.nature.com/articles/nature01492), which is used in the NIST group.\n\nWe will now discuss a bit the Soerensen-Moelmer gate. In this set-up two ions sit in a common trap. The cost of energy for exciting one of the ions will be\nlabelled $\\omega_t$ as in the first section. So we assume that the\nscheme starts in the state $\\left|ggn\\right\\rangle$, where\nboth atoms are in the internal ground-state $g$ and in some excited trap\nlevel $n$.\n\nIn the next step, these two ions experience two lasers, which are\ncoupling excited and the ground state of the ions:\n\n- One laser has frequency $\\omega_1=\\omega_0-\\omega_t+\\delta$ and Rabi\n  coupling strength $\\Omega$. It is therefore only slightly detuned from the transitions $|ggn\\rangle\\rightarrow|eg,n-1\\rangle |ge,n-1\\rangle$.\n\n- The second laser has frequency $\\omega_2=\\omega_0+\\omega_t-\\delta$ and Rabi coupling strength $\\Omega$. It is therefore only slightly detuned from the transitions $|ggn\\rangle\\rightarrow|eg,n+1\\rangle |ge,n+1\\rangle$.\n\nThe gate is then operated in the regime of small coupling strength\n$\\eta \\Omega n \\ll \\delta$. In this case coupling to the excited\nmotional states $n\\pm 1$ is suppressed by a factor of\n$\\frac{\\eta \\Omega n}{\\delta}$. On the other hand we are exactly on\nresonance for the two-photon transitions\n$|ggn\\rangle\\rightarrow|eg,n+1\\rangle\\rightarrow|ee,n\\rangle$ etc. So we\ncan do second-order pertubation theory or\nadiabatic elimination to obtain the\neffective Hamiltonian:\n\n$$\n\\begin{aligned}\nH_\\mathrm{SM} &= \\frac{\\Omega_\\mathrm{SL}}{2}\\left(\\left|ggn\\right\\rangle\\left\\langle een\\right| + (\\left|een\\right\\rangle\\left\\langle ggn\\right|\\right)\\text{ with }\\Omega_{SL} = -\\frac{(\\Omega \\eta)^2}{2(\\eta - \\delta)}\\end{aligned}\n$$\n\nSo starting out with the state $\\left|gg\\right\\rangle$ and\napplying the laser for $t\\Omega =\\frac{\\pi}{2}$, we obtain the entangled\nstate that we are looking for.\n\nThe operation of the gate was first demonstrated in 2000 by the Wineland\ngroup and allowed at the time for generating a Bell state with a\n[fidelity of 83%](https://www.nature.com/articles/35005011). This limit has been increasingly pushed of the years and now reaches the 99.9% region.\n\nSuch a fidelity sounds very impressive on first sight and it is by now\nthe result of several decades of work. However, in a quantum computer we\nwould like to chain a large number of these gates behind each other.\n\n- After 10 iterations the fidelity dropped to 99%.\n\n- After 100 iterations the fidelity dropped to 90%.\n\n- After 1000 iterations the fidelity dropped to 30%.\n\nSo even with such an excellent fidelity it will barely be possible to\nchain much more than 100 gates before the some extremely iffy things\nstart to happen.\n\nSo we have experimentally the choice of entanglement tool in the way\nthat is most adapted to our work.\n\n## Summary\n\nTrapped ions have now become a major platform for quantum computation and in this tutorial, we have discussed the main ingriedients that underlie the computation. If you would like to use the devices, there are now emerging three major competitors within this technology stack:\n\n- [IonQ](https://ionq.com/) is based in Maryland with Chris Monroe as one of the founders. It was one of the first pure quantum computation companies and is now publically traded at the NASDAQ.\n\n- [AQT](https://www.aqt.eu/) is a european company that is closely connected to the research team in Innsbruck around some of the founding fathers of the field like Rainer Blatt or Peter Zoller.\n\n- [Quantinuum](https://www.quantinuum.com/) has its traditions in the NIST Boulder region on the hardware side. Despite their slightly younger age as a company they have recently demonstrated very high fidelity processors and became one of the leading players.\n\nIn next weeks tutorial, we will move on to superconducting qubits and see how they are implementated.\n\n```python\n\n```",
    "order": 3
  },
  {
    "title": "Tutorial 4 - A few words about quantum computing with superconducting qubits",
    "content": "\nWe have seen in the [last tutorial](./2) the\npossibilities of quantum computation with trapped ions. However, a second major platform are superconducting qubits. They the platform of choice of commercial giants like [google](https://quantumai.google/), [IBM](https://www.ibm.com/quantum) or [Rigetti](https://www.rigetti.com/).\nIn this tutorial, we will identify the existence of qubits in superconducting circuits, the different gates and the read-out. We will finish by a comparison in the computing performance of trapped ions and\nsuperconducting qubits.\n\n## The quantum LC-oscillator\n\nAs in the second tutorial, we have to find the appropiate harmonic oscillator, but this time in electric circuits. Then we can discuss\nthe need of the Josephson junction for the implementation of\nsuperconducting qubits.\n\nThe fundamental ingredient for superconducting qubits are LC\noscillators, which are simply put a loop of wire which is not closed. To study\nits quantum behavior we will closely follow the discussion in Sec. II of\n[\"A Quantum Engineer's Guide to Superconducting Qubits\"](https://arxiv.org/abs/1904.06560v2).\n\nIn electrical engineering we first have to identify the conjugate\nvariables within the circuit. We will therefore follow the standard procedure of:\n\n1.  Identifying the equations of motion.\n\n2.  Identify the Lagrangien.\n\n3.  Identify the conjugate variables.\n\n4.  Write down the Hamiltonian.\n\n5.  Quantize the Hamiltonian.\n\nWhile it might be overly complicated for simple LC circuits it provides\na powerful framework for more complex systems (see [Nigg et al.](https://doi.org/10.1103/physrevlett.108.240502)\n)\n\n### Lagrangien formulation\n\nThe wire is caracterized by an _inductivity_, which is counteracting the\nchange in current:\n\n$$\n\\begin{aligned}\nV = L\\frac{dI}{dt}\\end{aligned}\n$$\n\nand a _capacitance_, which allows us\nto measure the cost of putting charges on the ends of the wire:\n\n$$\n\\begin{aligned}\nI = C\\frac{dV}{dt}\\end{aligned}\n$$\n\nTo put it under a partical form for\nquantization we typically express them through the flux, which is\ndefined as:\n\n$$\n\\begin{aligned}\n\\Phi(t) = \\int_{-\\infty}^tV(t')dt'\\end{aligned}\n$$\n\nThe electromagnetic\nenergy stored within the loop of wire is in general given by:\n\n$$\n\\begin{aligned}\nE(t) &= \\int_{-\\infty}^t V(t')I(t')dt'\\end{aligned}\n$$\n\nWe then obtain the\nenergies:\n\n$$\n\\begin{aligned}\nE_{kin} = \\frac{1}{2}C\\dot{\\Phi}^2\\\\\nE_{pot} = \\frac{1}{2L}\\Phi^2\\\\\\end{aligned}\n$$\n\nThis now leads to the\nLagrangien:\n\n$$\n\\begin{aligned}\nL &= \\frac{1}{2}C\\dot{\\Phi}^2-\\frac{1}{2L}\\Phi^2\\end{aligned}\n$$\n\n### Quantization\n\nWe can now identify the conjugate momentum the flux as:\n\n$$\n\\begin{aligned}\n\\frac{\\partial L}{\\partial\\dot{\\Phi}} &= C \\dot{\\Phi}\\\\\n&= Q\\end{aligned}\n$$\n\nSo the charge is the conjugate variable to the flux\nin the loop. They will be therefore the two fundamental variables of\nquantum theory, very much like position and momentum are for massive\nparticles.\n\nWe can now write down the Hamiltonian as:\n\n$$\n\\begin{aligned}\nH &= Q\\dot{\\Phi}- L\\\\\n&= \\frac{Q^2}{2C}+\\frac{\\Phi^2}{2L}\\end{aligned}\n$$\n\nAt this stage we can\nquantize the system through the commutation relation:\n\n$$\n\\begin{aligned}\n[\\hat{\\Phi},\\hat{Q}]&= i\\hbar\\\\\n\\hat{H} &= \\frac{\\hat{Q}^2}{2C}+\\frac{\\hat{\\Phi}^2}{2L}\\end{aligned}\n$$\n\nSo it is once again a harmonic oscillator with resonance frequency\n$\\omega_r = \\frac{1}{\\sqrt{LC}}$ and 'mass' $C$. So the system reads:\n\n$$\n\\begin{aligned}\n\\hat{H} &= \\hbar \\omega_r \\left(\\hat{a}^\\dagger a + \\frac{1}{2}\\right)\\end{aligned}\n$$\n\nWhile this is now a quantum system, it is manifestly not a qubit as the\ntransitions are equidistant in energy with $\\omega_r$. The typical order\nof magnitude is here 3-6GHz.\n\nTo prepare for the introduction of superconducting elements, we typically rewrite the equations above in terms of dimensionless quantities. Namely the Cooper pair density $n = \\frac{Q}{2e}$ and the reduced flux $\\phi= 2\\pi \\Phi/\\Phi_0$ with $\\Phi_0 = \\frac{h}{2e}$. These two quantities correspond directly to the density and the phase of the superconducting wavefunction that we will discuss in the next section. We then obtain the Hamiltonian $$\\begin{aligned} \\hat{H} = 4E_C n^2 + \\frac{1}{2}E_L \\varphi^2\\end{aligned}$$ So we quantify the influence of each lump element through their energy:\n\n- $E_C=\\frac{e^2}{2C}$ is the energy required to add a cooper pair.\n\n- $E_L=\\frac{(\\Phi_0/2\\pi)^2}{L}$ is the inductive energy\n\n## The Josephson junction\n\nTo resolve the degeneracy we need to make the oscillator anharmonic.\nThis is done through _Josephson junctions_, which are the backbone of\nsuperconducting electronics (very much like the transistor or the diode\nare classical electronics). To understand them roughly, we will fall\nback on the [Feynman\npicture](http://www.feynmanlectures.caltech.edu/III_21.html) of\nJosephson dynamics.\n\n### A simplistic picture of superconductivity\n\nWe could spend several lectures to understand the physics of Josephson\njunctions in all its gory details. A good overview might be found in the\nfollowing books [(D.R. Tilley, 1990](https://www.crcpress.com/Superfluidity-and-Superconductivity/Tilley-Tilley/p/book/9780750300339); [Tinkham, 2004)](https://books.google.de/books?id=VpUk3NfwDIkC). However, the basic idea is that the fermionic electrons form cooper\npairs at very low temperatures. These pairs are bosonic and can hence\ncondense into a macroscopic wavefunction: $$\\begin{aligned}\n\\psi(x,t) &= \\sqrt{n}e^{i\\varphi(x,t)}\\end{aligned}$$ Now the system can\nbe understood through the following relations:\n\n- the density is given by $n= |\\psi(x,t)|^2$.\n\n- The velocity is set by the gradient of the phase\n  $\\vec{v}= \\frac{\\hbar}{2m_e}\\nabla \\varphi$.\n\n- The voltage is set by the time evolution of the phase\n  $V = \\frac{\\hbar}{2e} \\frac{\\partial \\varphi}{\\partial t}$.\n\n### The Josephson relations\n\nA Josephson junction describes now a system where two superconducting\nregions are slightly separated by a normal metal such that only\ntunneling is allowed between the two regions.\n\nWe can now write down the SchrÃ¶dinger equation for this setup:\n\n$$\n\\begin{aligned}\ni\\hbar \\partial_t \\psi_L &= \\frac{eV}{2}\\psi_L+J \\psi_R\\\\\ni\\hbar \\partial_t \\psi_R &= -\\frac{eV}{2}\\psi_R+J \\psi_L\\end{aligned}\n$$\n\n$V$ is the voltage applied to the junction and $J$ is the tunneling\nelement. We now use the decomposition to write:\n\n$$\n\\begin{aligned}\n\\dot{n}_L &= \\frac{2}{\\hbar}J\\sqrt{n_Rn_L}\\sin(\\delta )\\\\\n\\dot{n}_R &= -\\frac{2}{\\hbar}J\\sqrt{n_Rn_L}\\sin(\\delta)\\\\\n\\phi &=\\varphi_L-\\varphi_R\\end{aligned}\n$$\n\nWe can now use it to write\ndown the current-phase relationship:\n\n$$\n\\begin{aligned}\nI &= I_c\\sin(\\phi)\\\\\nI_c &= \\frac{2}{\\hbar}Jn\\end{aligned}\n$$\n\nWe can once again integrate the\nequation of motion to obtain:\n\n$$\n\\begin{aligned}\n\\hat{H} = 4E_Cn^2-E_J \\cos(\\phi)\\\\\nE_C = \\frac{e^2}{2(C+C_J)}\\\\\nE_J = \\frac{I_C\\Phi_0}{2\\pi}\\end{aligned}\n$$\n\n## Single qubit control\n\nSuperconducting qubits can be controlled either through inductive or\ncapacitive coupling. Inductive coupling is widely used for flux-qubits\nlike the rf-squid. However, here we focus on the transmon qubit, which\nis typically capacitavely coupled\n\nGoing through the quantization procedure we discussed above, we can write\nthe circuit Hamiltonian as:\n\n$$\n\\begin{aligned}\n\\hat{H} &= \\frac{\\tilde{Q}^2}{2C_\\Sigma}+\\frac{\\Phi^2}{2L}+\\frac{C_d}{C_\\Sigma}V_d(t)\\tilde{Q}\\end{aligned}\n$$\n\nThe charge is defined for this system as\n$\\tilde{Q} = C_\\Sigma\\dot{\\Phi} - C_d V_d(t)$. In the limit of weak\ncoupling $C_d V_d \\ll C_\\Sigma \\dot{\\Phi}$, we have can quantize the\nsystem as before and only need to understand the influence of the last\nterm.\n\nThe second part of the Hamiltonian resembles strong the electric dipole\ncoupling we discussed in the last lecture. It contains the displaced\ncharge, which is linearly coupled to an oscillating electric field. So\nwe can rewrite the charge once again in terms of raising an lower\noperators and arrive at the coupled Hamiltonian:\n\n$$\n\\begin{aligned}\n\\hat{H} &= \\frac{\\omega_t}{2}\\hat{\\sigma}_z+\\Omega V_d(t)\\hat{\\sigma}_y\\end{aligned}\n$$\n\nAll the other discussions are equivalent to our discussion on the ion\nand any other single qubit system.\n\n## Generating entanglement\n\nHaving identified the qubit, we can now also implement the entanglement\ngate to build the universal quantum computer. Different options exist:\n\n- The qubit island could be coupled through a mutual capacitance, such\n  that $\\hat{H}_{int}= C_g V_1 V_2$.\n\n- The qubit island could be coupled through a mututal inductance, such that $\\hat{H}_{int}= M_{12} I_1 I_2$.\n\nTypically the inductive coupling is chosen in a regime of very small\ncoupling $C_g \\ll C_1, C_2$, where the $C_i$ describe the transmon\nqubits. The full Hamiltonian reads then:\n\n$$\n\\begin{aligned}\n\\hat{H} &=\\hat{H}_{T,1}+\\hat{H}_{T,2}+4e^2\\frac{C_g}{C_1C_2}n_1n_2\\end{aligned}\n$$\n\nwe identified here $V_i = \\frac{2e}{C_i}n_i$. We can now further rewrite\nthe occupation in terms of raising and lowering operators\n$n\\propto i(a-a^\\dagger)$, which is can be expressed as a Pauli matrix for\nthe buttom manifold. So we actually have the coupling:\n\n$$\n\\begin{aligned}\n\\hat{H} &=\\hat{H}_{T,1}+\\hat{H}_{T,2}+g\\sigma_{y,1}\\sigma_{y,2}\\end{aligned}\n$$\n\nWhile this basic operating principle of capacitive coupling is indeed\nwidely used, it is worth to read the fine-print as the different actual\nimplementation can to different 2-Qubit gates:\n\n- The **iSWAP** gate is the implementation of the $\\sigma_y \\sigma_y$\n  coupling.\n\n- The **phase** gate, shows very high fidelities, but makes it necessary to tune the freuqency of the qubit. It implements a $\\sigma_z \\sigma_z$-coupling on the spins. Fidelities of \\> 99% were demonstrated for this gate [(Barends et al., 2014)](https://doi.org/10.1038/nature13171).\n\n- The **cross-resonance** (CR) gate is only controlled through\n  microwaves. It implements the a $\\sigma_x \\sigma_z$-coupling on the\n  spins. This is gate employed by IBM [(Chow et al., 2011)](https://doi.org/10.1103/physrevlett.107.080502).\n\n## A CNOT gate constructed from the physical entangling gates\n\nWe would now like to discuss how we can use the capacitive coupling to\nimplement a CNOT gates. This discussion is closely related to the\npossibility of using a Soerensen-Molmer gate, discussed in lecture 2, to\nimplement a CNOT gate.\n\nThe $\\sigma_y \\sigma_y$ coupling for the right amount of leads to a\ncoupling matrix:\n\n$$\n\\begin{aligned}\niSWAP &= \\left(\\begin{array}{cccc}\n1 & 0 & 0 & 0\\\\\n0 & 0 &-i & 0\\\\\n0 & -i & 0 & 0\\\\\n0 & 0 & 0 & 1\n\\end{array}\\right)\\end{aligned}\n$$\n\nThe iSWAP can then be used to\nrepresent the CNOT gate.\n\nSo we will focus shortly on the current limitations of qubit systems.\nPlease be aware that this is a rapidly evolving field, so most likely\nthe paragraph will be outdated within a few months. One good summary can\nbe found in [(Linke et al., 2017)](https://doi.org/10.1073/pnas.1618020114) and written by an ion trapping group.\n\n## Summmary\n\nIn this tutorial, we discussed how the two level system is implemented in superconducting devices and how they are coupled. This should give you a basic feeling for this widely used hardware platform and its limitations.",
    "order": 4
  },
  {
    "title": "Tutorial 5 -Quantum simulation with cold atoms",
    "content": "\nIn the previous lessons we talked about [trapped ions](qhw3) and [superconducting qubits](qhw4). They are the leading platforms for the creation of **universal quantum computers**, which hold the premise of exponential speed-up for a variety of NP-hard problems. However, this universality usually comes at the price of algorithmic overhead for a wide range of tasks which generally leads to worse overall fidelity. If we give up the constraint of a universal quantum computer, we can build specialized quantum hardware for the problems of interest. Such systems are calledÂ quantum simulators. Cold atomsÂ have becomeÂ a leading platform of such quantum simulators at a large scale. Already demonstrated applications involve an enormous variety of condensed-matter problems like e.g. the [Hubbard model](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.80.885), topological systems, [superfluidity](https://www.sciencedirect.com/science/article/abs/pii/S1049250X06540017?via%3Dihub) and [disorder](https://physicstoday.scitation.org/doi/10.1063/1.3206091). More recently, cold atoms started to find applications to [Ising models](https://arxiv.org/abs/2002.07413) and [high-energy physics](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2021.0064).Â \n\n## Summary for the cold atoms hardwareÂ \n\nUltracold atoms are gases that are well isolated from the environment and are cooled to near zero and in this regard quite close to trapped ion systems, which also heavily draw on the experimental techniques of atomic physics and optics.Â At such low temperatures, quantum effects are dominant in the behavior of the gas cloud.Â Thanks to their precise control and flexibility, they are now a well-established platform for the quantum simulation of complex quantum problems.Â \n\nThe high precision and versatility have made cold atoms one of the dominating quantum hardware platforms in the European academic research sector. To underline the fact, in Heidelberg alone there are four research groups working with cold atoms. Munich, Hamburg and Bonn have similar numbers. France has at least 20 research groups on cold atoms, 1 on trapped ions and a handful on superconducting qubits. Similar numbers hold for Italy.\n\n## Qubits based on Rydberg atoms\n\n- **Use case**: The same as trapped ions and superconducting qubits\n- **Companies**: Pasqal, Quera, Atom Computing, ColdQuanta, planqc\n- **Research Groups**: Paris, Harvard, Caltech, Munich, Heidelberg, â¦Â \n\nThe situation, which is closest to superconducting qubit systems are Rydberg quantum simulators. In these systems the atoms are confined in optical tweezer and strongly interact when they are excited into Rydberg states , as such implementing interacting qubit systems. They share many of features, which are similar to ions and superconducting qubits. The Hamiltonian, which is implemented in these systems, is a complex Ising type model:\n\n$H=\\sum_i\\hat{\\sigma}_{x,i}+\\sum_iU_{ij}\\hat{n}_i\\hat{n}_j $, with $\\hat{\\sigma}_{x,i}=| râ©â¨gâ|_i+| gâ©â¨râ|_i$ and $\\hat{n}_i=|râ©â¨râ|_i$.\n\nThey are currently, well established candidates for the quantum simulation of maximum-independent set problems, which already let to the foundation of the start-ups [Pasqal](https://pasqal.io/), [Quera](https://www.quera.com/), [Atom Computing](https://atom-computing.com/), [ColdQuanta](https://coldquanta.com/) or [planqc](https://www.planqc.eu/).\n\n## Itinerant particlesÂ built from cold atoms\n\n- **Use case**: Condensed matter, chemistry\n- **Research Groups**: Munich, Heidelberg, Zurich, Harvard, â¦Â \n- **Representable as circuit**: Yes\n\nThe most widely used configuration of cold atoms are moving atoms in some external optical potential. The atomic clouds are typically trapped by focused laser beams, which can be shaped in a flexible fashion. As such a wide variety of systems exists:\n\n- Traps with hundreds of thousands of atoms, which emulate superfluid behavior and Bose-Einstein condensation.\n- Traps with hundreds of atoms for the study of continuous variables and large-scale entanglement.\n- Optical lattices from the interference of laser beams are now widely used for the study of lattice models in condensed matter with system sizes well above the 100 sites and atoms.\n\nAdditionally, the quantum statistics can be tuned widely:\n\n- Working with fermionic or bosonic isotopes the atoms can directly emulate fermionic or bosonic problems.\n- If a large number of bosons are condensed into a single spatial mode they can form large collective spins, i.e. continuous variables, from their internal degrees of freedom.\n- The connection between moving particles and spin degrees leads to a variety of exotic behaviors like topological band structures, if needed.\n\nThe particles are interacting in various ways:\n\n- Most Alkali based system are described through a simple contact interaction.\n- Alkaline-earth atoms and Rydberg systems have long range interactions.\n\nThe readout of the system can be performed in various fashion:\n\n- Spatially resolved fluorescence imaging provides a destructive, state-sensitive readout.Â \n- Depending on the measurement protocol momentum or position can be measured.\n- [Dispersive measurements allow for weak measurement protocols.\n\nIf the external potential is an optical lattice they provide a synthetic realization of strongly correlated lattice models as visualized in Figure 2.\n\nNowadays, the systems are used for the study of quantum dynamics with quench protocols very similar to quantum computation systems as visualized on the right-hand side of the figure. Those experiments realize the Hubbard model:\n\n$H= -J\\sum_i(\\hat{c}_{i+1}^\\dagger \\hat{c}_i+h.c.) +\\frac{U}{2}\\sum_i \\hat{n}_i(\\hat{n}_i-1)$\n\nAs such they have been widely employed for the study of many-body problems, which come originally from condensed matter. In the early stages this was used to study ground state properties of complex systems like unitary fermions or strongly correlated lattice systems. Importantly, experiments have gone well into the regime of non-perturbative theories. They are therefore well-established benchmarks for sophisticated numerical techniques, which would be otherwise uncontrolled.\n\nCurrently, this kind of quench experiments have to be completely programmed at the hardware level. However, with [qiskit-cold-atoms](https://qiskit-extensions.github.io/qiskit-cold-atom/tutorials/03_fermionic_tweezer_hardware.html) it is now possible to emulate such problems and a hardware integration seems only like a question of time.\n\n## Application to quantum chemistry\n\nAs IBM already demonstrated beautifully in previous experiments, molecular systems are made for the study of NISQ devices. However, the translation of the fermionic electrons on Qubits currently involves complex transformations like the Jordan-Wigner transformation. These can lead to substantial overhead and substantially increase the required circuit depth. Using fermionic atoms in microtraps, we directly emulate the chemistry problem onto the hardware.\n\nIn these systems the molecule is translated into its orbital basis. Each orbit is then represented by the occupation of a fermion of an optical tweezer. These fermionic systems are described by a hopping term HJand a non-commuting interaction term HU. As such they become a potential resource for variational algorithms, which again would heavily benefit from an integration.\n\nThese systems are also excellent candidates for the quantum simulation of chemistry problems as they naturally realize interacting fermions on different spatial orbits.\n\nMixed integer problems in atomic mixtures\n\nUse cases: High-energy physics. Power plant optimization\n\nResearch Groups: MIT, Heidelberg, Karlsruhe, Harvard, JILAÂ \n\nAtomic mixtures are an extension to cold atoms as the provide the ability to let two independent atomic species interact. As such they are an ideal platform to study complex problems, which are defined on complex computational sectors as we describe for lattice gauge theories.Â \n\nIn the fundamental laws of physics, gauge fields mediate the interaction between charged particles. An example is quantum electrodynamicsâthe theory of electrons interacting with the electromagnetic fieldâbased on U(1) gauge symmetry. Solving such gauge theories is in general a hard problem for classical computational techniques. While quantum computers suggest a way forward, it is difficult to build large-scale digital quantum devices required for complex simulations. In cold atoms, we work on a fully scalable analog quantum simulator of a U(1) gauge theory in one spatial dimension. Some experiments demonstrated the experimental realization of the elementary building block as a key step towards a platform for large-scale quantum simulations of continuous gauge theories.\n\nIn the experiments one atomic species emulates the gauge fields described by the Hamiltonian $H_g$ and the other species the matter field with Hamiltonian $H_m$, interacting through a coupling Hamiltonian $H_{mg}$. In the typical experiments the gauge field is tuned through an x-rotation and then the system is left to evolve under the full Hamiltonian $H=H_g+H_m+H_{mg}$ :\n\n$H= \\chi \\sum_ i\\hat{L}_{z,i}^2+M\\sum_i (-1)^i\\hat{n}_i+J\\sum_i(\\hat{c}_{i+1}^\\dagger\\hat{L}_{+,i}\\hat{c}_i+h.c. )$\n\nThese Hamiltonian are extremely demanding computationally as the gauge field is defined on the Hilbert space of a [continuous variable](qhw2) $L$ and the matter field on the Hilbert space of itinerant fermions. As such it presents a physical example of a NP-hard mixed-integer problem.\n\nAgain, this kind of experiments have to be completely programmed at the hardware level. Quite importantly, this translation currently requires a good knowledge of the hardware from all participants, even high-energy physicists. This makes the experiments currently extremely cumbersome to translate from atomic physics to high-energy physics and back. As such they are one of the interesting use cases that drove the development of [qiskit-cold-atoms](https://github.com/Qiskit-Extensions/qiskit-cold-atom).\n\n## Machine learning and power plant optimization\n\nMachine learning algorithms heavily draw on similar problems like chemistry as they attempt to optimize energy functionals. However, in those problems some of the variables are typically [continuous variables](qhw2) and some [binary](qhw1). These mixed-integer optimization problems are NP hard to solve on classical hardware. The continuous variables on the other hand make them extremely resource intensive for qubit-based systems. Cold atoms show a way forward to employ continuous variables and qubits directly into the system and employ a variational algorithm on them.Â Similar problems arise for power plant optimizations, when the power plant can give certain power outputs above a certain threshold. Both topics are under active investigation, but currently suffer from the lack of an intermediate programming layer between the users and the hardware developers.\n\n## Summary\n\nThis is the last lesson of this course. You learned:\n\n- That cold atoms have become one of the leading quantum simulation platforms.\n- Rydberg atoms can implement qubit systems, which are similar to trapped ions of superconducting qubits.\n- Cold atoms can easily tackle complex problems with itinerant particles.",
    "order": 5
  }
]
