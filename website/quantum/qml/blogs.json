[
  {
    "title": "QML 001 - A summary of classical supervised learning",
    "content": "\nIn this tutorial, we will discuss some basic ideas behind classical supervised learning before we jump into the quantum part.\n\nThe notebook is structured as follows:\n\n- We introduce the learning task of classifying data points $x_i$ with labels $y_i$\n- We introduce simple classification through logistic regression with bias and weight.\n- We provide simple training\n- We test the performance of the circuit.\n\nWe will always focus on simplicity throughout this tutorial and leave the more complex discussions to the extensive literature. So readers thank think \"yet another tutorial on logistic regression\", can most likely directly jump to the very [first tutorial](qml101) on quantum machine learning.\n\n## A simple learning task\n\nFor simplicity we will start out with a simple problem, where each data set has only a single variable and extend it later to higher dimensional data sets.\n\n```python\nfrom typing import Union, List\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\n```\n\n```python\nnp.random.seed(1)\nx = np.random.uniform(-np.pi, np.pi, 100)\ny = 1.0* (x <  1)\n\nf, ax = plt.subplots()\nax.plot(x, y, \"o\")\nax.set_xlabel(r\"input value $x_i$\");\nax.set_ylabel(r\"label $y_i$\");\n```\n\n![png](./qml_001_3_0.png)\n\nThe learning task is now to predict the label $y_i$ from the input value $x_i$. To get started we have to divide the data set into a training part and a test part:\n\n- On the _training set_ we will optimize the algorithm to achieve the highest possible accuracy in predicting the label.\n- On the _test set_ we will test the performance of the algorithm with data it has never seen.\n\nThe usual problem is here to find a good balance between a sufficient amount of training data, yet leaving enough test data to have a statistically significant test.\n\n```python\nfrom sklearn.model_selection import train_test_split\n```\n\n```python\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.20, random_state=42\n)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\nax1.plot(x_train, y_train, \"o\")\nax1.set_xlabel(\"input value\")\nax1.set_ylabel(\" given labels\")\nax1.set_title(\"training data\")\n\nax2.plot(x_test, y_test, \"o\")\nax2.set_xlabel(\"input value\")\nax2.set_title(\"test data\")\n```\n\n    Text(0.5, 1.0, 'test data')\n\n![png](./qml_001_6_1.png)\n\n## Logistice regression as a minimal algorithm\n\nIt is now time to set up the algorithm for the training. We will use [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), despite the fact that this horse has been ridden to death. It is has just all the right ingredients that are necessary to agree on basic concepts and notations. The logistic function itself is defined as:\n\n$$\np(x) = \\frac{1}{1+e^{-(W\\cdot x+b)}}\n$$\n\nIt has a number of useful properties for us:\n\n- It interpolates nicely between 0 and 1.\n- The value of the transition is set by the _bias_ $b$. For $x \\gg b$ the exponential goes to zero and $p(x) \\rightarrow 1$, while it goes to 0 for the other side.\n- The sharpness of the transition is set by the weight $W$, which tells us how much of an influence we should attach to the input value $x$.\n\nBelow you can find a general example of such a logistic regression.\n\n```python\nweight = 3\nbias = 1\n\ny_log = 1/(1+np.exp(-(weight*x+bias)))\nf, ax = plt.subplots()\nax.plot(x, y_log, 'o')\nax.set_xlabel('x_i')\nax.set_ylabel('p(x)')\nax.set_title(\"logistic regression\");\n```\n\n![png](./qml_001_8_0.png)\n\nWe can use this logistic regression for labelling, but simply deciding that the label is 0, if $p(x) <\\frac{1}{2}$ and and 1 if $p(x) > \\frac{1}{2}$.\n\n```python\ndef get_accuracy(weight: float, bias: float, xvals: List[float], yvals: List[int]\n) -> Union[float, List[int]]:\n    \"\"\"\n    Calculates the accuracy of the logistic regression for a given set of data.\n\n    Args:\n      weight: the training parameter for the weight\n      bias: the training parameter for the bias\n      xvals: the input values\n      yvals: the labels\n    Returns:\n      The accuracy and the predicted labels.\n    \"\"\"\n    pred_labels = np.zeros(len(xvals))\n    accurate_prediction = 0\n    for ii, xinput, yinput in zip(range(len(xvals)), xvals, yvals.astype(int)):\n        # set the circuit parameter\n        y_log = 1/(1+np.exp(-(weight*xinput+bias)))\n        pred_label = 1.0*(y_log>1/2)\n        pred_labels[ii] = pred_label\n        if yinput == pred_label:\n            accurate_prediction += 1\n    return accurate_prediction / len(yvals), pred_labels\n```\n\nAnd now we can have a look at the labeling with some randomly guessed initial values.\n\n```python\nweight = -0.8\nbias = 0\naccuracy, y_pred = get_accuracy(bias = bias, weight=weight, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nprint(f\"The circuit has an accuracy of {accuracy}\")\nf, ax = plt.subplots()\nax.plot(x_train, y_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n```\n\n    The circuit has an accuracy of 0.85\n\n\n\n\n\n    <matplotlib.legend.Legend at 0x7f75c9992110>\n\n![png](./qml_001_12_2.png)\n\nAs we can see above there is quite a regime, where the model does to predict the labels very well. This can be improved by training the model parameters systematically.\n\n## Training the minimalistic algorithm\n\nTo improve the performance of the circuit, we have to train it. This basically involves the minimization of some loss function as a function of the circuit parameters $W$ and $b$. In this example, we can simply calculate the accuracy of the circuit as a function of the bias and obtain its minimimum.\n\n```python\nweight = -1\n\nNbias = 101\nbiases = np.linspace(-2, 2, Nbias)\naccuracies = np.zeros(Nbias)\n\nfor ii, bias_val in enumerate(tqdm(biases)):\n    accuracy, y_pred = get_accuracy(bias = bias_val, weight=weight, xvals=x_train, yvals=y_train)\n    accuracies[ii] = accuracy\n```\n\n    100%|██████████| 101/101 [00:00<00:00, 761.27it/s]\n\n```python\nopt_bias = biases[accuracies.argmax()]\n\nf, ax = plt.subplots()\nax.plot(biases, accuracies)\nax.axvline(opt_bias, color=\"C1\", label=\"optimal bias\")\nax.set_ylabel(\"accuracy\")\nax.set_xlabel(\"biases\")\nax.legend()\n```\n\n    <matplotlib.legend.Legend at 0x7f75bd886f10>\n\n![png](./qml_001_16_1.png)\n\nWe clearly identify a optimal value for the bias at which the accuracy is maximal. This allows to test the accuracy on the optimal value of the weights again to obtain.\n\n```python\naccuracy, y_pred = get_accuracy(bias = opt_bias , weight= weight, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nf, ax = plt.subplots()\nax.plot(x_train, y_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n\n\nprint(f\"The trained circuit has an accuracy of {accuracy:.2}\")\n```\n\n    The trained circuit has an accuracy of 1.0\n\n![png](./qml_001_18_1.png)\n\n## Testing the algorithm\n\nHaving finished the training, we can test the circuit now on data points that it has never seen.\n\n```python\ntest_accuracy, y_test_pred = get_accuracy(\n    bias = opt_bias, weight=weight, xvals=x_test, yvals=y_test\n)\n\nfalse_label = abs(y_test_pred - y_test) > 0\n\nx_false = x_test[false_label]\ny_false = y_test_pred[false_label]\n\nprint(f\"The circuit has a test accuracy of {test_accuracy:.2}\")\nf, ax = plt.subplots()\nax.plot(x_test, y_test_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend();\n```\n\n    The circuit has a test accuracy of 1.0\n\n![png](./qml_001_20_1.png)\n\n## Summary of classical supervised learning\n\nIn this tutorial, we studied some basic concepts like training and classification for an extremely simple case. We saw:\n\n- the existence of a classification algorithm.\n- How it relates input and output label.\n- How it is trained.\n- How it is tested on test data.\n\nIn the [first tutorial on QML](https://colab.research.google.com/drive/1XMkIBrU1lBLTT-oVufVTifHivss0HDI1?usp=sharing), we will see how this translate in the simplest fashion to quantum algorithms.",
    "order": 0
  },
  {
    "title": "QML 101 - Some basic concepts",
    "content": "\nThis is the second part of our beginners guide to quantum machine learning. In the last part we introduced some basic notions of classical supervised learning, on which we will build up on. In this tutorial, we will discuss some basic ideas behind quantum machine learning. Hereby, we focus on classifiers, which are part of the big class of supervised (deep) learning algorithms.\n\nThe notebook is structured as follows:\n\n- We introduce the learning task of classifying data points $x_i$ with labels $y_i$\n- We introduce the circuit structure and the cost function.\n- We provide simple training\n- We test the performance of the circuit.\n\nWe will always focus on simplicity throughout this tutorial and leave the more complex discussions to the extensive literature.\n\n## A simple learning task\n\nFor simplicity we will start out with a simple problem, where each data set has only a single variable and extend it later to higher dimensional data sets.\n\n```python\n# only necessary on colab\n\n!pip install qiskit\n!pip install pylatexenc\n```\n\n```python\nfrom typing import Union, List\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\n```\n\n```python\nnp.random.seed(1)\nx = np.random.uniform(-np.pi, np.pi, 100)\ny = 1.0 * (abs(x) > np.pi / 2)\n\nf, ax = plt.subplots()\nax.plot(x, y, \"o\")\nax.set_xlabel(\"input value\")\nax.set_ylabel(\"label\")\n```\n\n    Text(0, 0.5, 'label')\n\n![png](./qml_101_4_1.png)\n\nThe task will then be to predict the label $y_i$ from the input value $x_i$. To get started we have to divide the data set into a training part and a test part:\n\n- On the _training set_ we will optimize the algorithm to achieve the highest possible accuracy in predicting the label.\n- On the _test set_ we will test the performance of the algorithm with data it has never seen.\n\nThe usual problem is here to find a good balance between a sufficient amount of training data, yet leaving enough test data to have a statistically significant test.\n\n```python\nfrom sklearn.model_selection import train_test_split\n```\n\n```python\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.20, random_state=42\n)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\nax1.plot(x_train, y_train, \"o\")\nax1.set_xlabel(\"input value\")\nax1.set_ylabel(\" given labels\")\nax1.set_title(\"training data\")\n\nax2.plot(x_test, y_test, \"o\")\nax2.set_xlabel(\"input value\")\nax2.set_title(\"test data\")\n```\n\n    Text(0.5, 1.0, 'test data')\n\n![png](./qml_101_7_1.png)\n\n## A first algorithm\n\nIt is now time to set up the algorithm for the training. Given that there are only two labels, we can directly work with a single qubit. The algorithm is then of the following structure:\n\n1. Prepare the initial state.\n2. Apply a parametrized circuit with parameters $\\mathbf{w}$ that depend on the input $U(\\mathbf{w}, x_i)$.\n3. Read out the label from the measurement of the qubit.\n\nIn the simplest of all cases, we might then just apply a single qubit rotation with the angle $w x_i$\n\n```python\nfrom qiskit.circuit import QuantumCircuit, Parameter\nfrom qiskit import Aer\n\nsim = Aer.get_backend(\"aer_simulator\")\n```\n\n```python\ntheta = Parameter(r\"$\\theta$\")\n\nqc = QuantumCircuit(1)\nqc.rx(theta, 0)\nqc.measure_all()\nqc.draw(\"mpl\")\n```\n\n![png](./qml_101_10_0.png)\n\nWe can now look at the performance of the code with some randomly initialized weight in predicting the appropiate label.\n\n```python\ndef get_accuracy(\n    qc: QuantumCircuit, weight: float, xvals: List[float], yvals: List[int]\n) -> Union[float, List[int]]:\n    \"\"\"\n    Calculates the accuracy of the circuit for a given set of data.\n\n    Args:\n      qc: the quantum circuit\n      weight: the training parameter\n      xvals: the input values\n      yvals: the labels\n    Returns:\n      The accuracy and the predicted labels.\n    \"\"\"\n    pred_labels = np.zeros(len(xvals))\n    accurate_prediction = 0\n    for ii, xinput, yinput in zip(range(len(xvals)), xvals, yvals.astype(int)):\n        # set the circuit parameter\n        circuit = qc.assign_parameters(\n            {theta: weight * xinput},\n            inplace=False,\n        )\n        # run the job and obtain the counts\n        job = sim.run(circuit, shots=1000)\n        counts1 = job.result().get_counts()\n\n        # obtain the predicted label on average\n        if \"0\" in counts1:\n            pred_label = 1 * (counts1[\"0\"] < 500)\n        else:\n            pred_label = 1\n        pred_labels[ii] = pred_label\n        if yinput == pred_label:\n            accurate_prediction += 1\n    return accurate_prediction / len(yvals), pred_labels\n```\n\n```python\nweight = 0.8\n\naccuracy, y_pred = get_accuracy(qc, weight=weight, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nprint(f\"The circuit has an accuracy of {accuracy}\")\nf, ax = plt.subplots()\nax.plot(x_train, y_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n```\n\n    The circuit has an accuracy of 0.9\n\n\n\n\n\n    <matplotlib.legend.Legend at 0x7ff478a0ef70>\n\n![png](./qml_101_13_2.png)\n\n## Training\n\nTo improve the performance of the circuit, we have to train it. This basically involves the minimization of some loss function as a function of the circuit parameters $\\mathbf{w}$. In this example we can simply calculate the accuracy of the circuit as a function of the weight and obtain its minimimum.\n\n```python\nweights = np.linspace(0, 2, 50)\naccuracies = np.zeros(50)\n\nfor ii, weight_val in enumerate(tqdm(weights)):\n    accuracy, y_pred = get_accuracy(qc, weight=weight_val, xvals=x_train, yvals=y_train)\n    accuracies[ii] = accuracy\n```\n\n    100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  8.14it/s]\n\n```python\nopt_weight = weights[accuracies.argmax()]\n\nf, ax = plt.subplots()\nax.plot(weights, accuracies)\nax.axvline(opt_weight, color=\"C1\", label=\"optimal weight\")\nax.set_ylabel(\"accuracy\")\nax.set_xlabel(\"weights\")\nax.legend()\n```\n\n    <matplotlib.legend.Legend at 0x7ff4aca197f0>\n\n![png](./qml_101_16_1.png)\n\nWe can now test the accuracy on the optimal value of the weights again to obtain.\n\n```python\naccuracy, y_pred = get_accuracy(qc, weight=opt_weight, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nf, ax = plt.subplots()\nax.plot(x_train, y_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n\n\nprint(f\"The trained circuit has an accuracy of {accuracy:.2}\")\n```\n\n    The trained circuit has an accuracy of 1.0\n\n![png](./qml_101_18_1.png)\n\n## Test\n\nHaving finished the training, we can test the circuit now on data points that it has never seen.\n\n```python\ntest_accuracy, y_test_pred = get_accuracy(\n    qc, weight=opt_weight, xvals=x_test, yvals=y_test\n)\n\nfalse_label = abs(y_test_pred - y_test) > 0\n\nx_false = x_test[false_label]\ny_false = y_test_pred[false_label]\n\nprint(f\"The circuit has a test accuracy of {test_accuracy:.2}\")\nf, ax = plt.subplots()\nax.plot(x_test, y_test_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n```\n\n    The circuit has a test accuracy of 1.0\n\n\n\n\n\n    <matplotlib.legend.Legend at 0x7ff4acb05cd0>\n\n![png](./qml_101_20_2.png)\n\n## Outlook for supervised learning\n\nIn this tutorial, we studied some basic concepts like training and classification for an extremely simple case.\n\n- In the [next tutorial](qml102), we will see how deeper circuits allow us to learn more complex dependencies between input and output.\n- The extension towards higher dimensional inputs allows us to look into more complext tasks like image processing.",
    "order": 1
  },
  {
    "title": "QML 102 - Deeper Classifiers",
    "content": "\nIn the [last tutorial](./1) we saw the most basic ideas of quantum machine learning algorithms. They included:\n\n- division in training and test data\n- simple training\n- accuracy tests.\n\nHowever, all of this happened in an extremely simple case, which allowed us to work with simple algorithms. In this tutorial, we will discuss the possibility to learn more complicated structures with deeper circuits. As in the last tutorial, the task will be once again the classification of data points $x_i$ with labels $y_i$. However the structure will be more complicated, such that the previous circuits would fail. So in this tutorial we will learn:\n\n- deeper reuploading circuits\n- training with optimizers.\n- All circuits will be implemented with `qiskit` because of its wide ranging use.\n\nWe will always focus on simplicity throughout this tutorial and leave the more complex discussions to the extensive literature and later tutorials.\n\n```python\n# only necessary on colab to have all the required packages installed\n\n!pip install qiskit\n!pip install pylatexenc\n```\n\n## The learning task\n\nAs previously, we will focus on a problem with one dimensional input data, which we associate with a label {0, 1}. However, the data set has a strucute, which will require deeper circuits.\n\n```python\nfrom typing import Union, List\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\n# for splitting the data set\nfrom sklearn.model_selection import train_test_split\n\n# for the quantum circuits\nfrom qiskit.circuit import QuantumCircuit, Parameter\nfrom qiskit import Aer\n```\n\n```python\nnp.random.seed(1)\nx = np.random.uniform(-np.pi, np.pi, 100)\ny = 1.0 * (abs(x) > 1.4*np.pi/2)\n\nf, ax = plt.subplots()\nax.plot(x, y, \"o\")\nax.set_xlabel(\"input value\")\nax.set_ylabel(\"label\")\n```\n\n    Text(0, 0.5, 'label')\n\n![png](./qml_102_4_1.png)\n\nOnce again we split the data set and get into the training.\n\n```python\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.20, random_state=42\n)\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\nax1.plot(x_train, y_train, \"o\")\nax1.set_xlabel(\"input value\")\nax1.set_ylabel(\" given labels\")\nax1.set_title(\"training data\")\n\nax2.plot(x_test, y_test, \"o\")\nax2.set_xlabel(\"input value\")\nax2.set_title(\"test data\")\n```\n\n    Text(0.5, 1.0, 'test data')\n\n![png](./qml_102_6_1.png)\n\n## A deeper algorithm\n\nTo achieve training for this more complex data set we have to work with a [data-reuploading approach](https://quantum-journal.org/papers/q-2020-02-06-226/). It is similiar to our previous approach as it does the following.\n\n1. Prepare the initial state.\n2. Apply a parametrized circuit with parameters $\\mathbf{w}$ that depend on the input $U(\\mathbf{w}, x_i)$.\n3. Read out the label from the measurement of the qubit.\n\nHowever, the circuit has now a structure, where input parameter is is applied again and again, interleaved with some processing layer. This allows for non-trivial classification tasks, which was analyzed in great detail [here](https://journals.aps.org/pra/abstract/10.1103/PhysRevA.103.032430).\n\n```python\nsim = Aer.get_backend(\"aer_simulator\")\n```\n\n```python\ntheta = Parameter(r\"$\\theta$\")\nalpha1 = Parameter(r\"$\\alpha_1$\")\nqc = QuantumCircuit(1)\nqc.rx(theta, 0)\nqc.rz(alpha1, 0)\nqc.rx(theta, 0)\nqc.measure_all()\nqc.draw(\"mpl\")\n```\n\n![png](./qml_102_9_0.png)\n\nWe can now look at the performance of the code with some randomly initialized weight in predicting the appropiate label.\n\n```python\ndef get_accuracy(\n    qc: QuantumCircuit, alpha: float, weight: float,\n    xvals: List[float], yvals: List[int]\n) -> Union[float, List[int]]:\n    \"\"\"\n    Calculates the accuracy of the circuit for a given set of data.\n\n    Args:\n      qc: the quantum circuit\n      alphas: the training parameter\n      weights: the weights for the inputs\n      xvals: the input values\n      yvals: the labels\n    Returns:\n      The accuracy and the predicted labels.\n    \"\"\"\n    pred_labels = np.zeros(len(xvals))\n    accurate_prediction = 0\n    for ii, xinput, yinput in zip(range(len(xvals)), xvals, yvals.astype(int)):\n        # set the circuit parameter\n        circuit = qc.assign_parameters(\n            {theta: weight*xinput,\n             alpha1: alpha,\n             },\n            inplace=False,\n        )\n        # run the job and obtain the counts\n        Nshots = 4000\n        job = sim.run(circuit, shots=Nshots)\n        counts1 = job.result().get_counts()\n\n        # obtain the predicted label on average\n        if \"0\" in counts1:\n            pred_label = 1 * (counts1[\"0\"] < Nshots/2)\n        else:\n            pred_label = 1\n        pred_labels[ii] = pred_label\n        if yinput == pred_label:\n            accurate_prediction += 1\n    return accurate_prediction / len(yvals), pred_labels\n```\n\n```python\nweight = 1\nalpha = np.pi/4\n\naccuracy, y_pred = get_accuracy(qc, alpha=alpha, weight=weight, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nprint(f\"The circuit has an accuracy of {accuracy}\")\nf, ax = plt.subplots()\nax.plot(x_train, y_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n```\n\n    The circuit has an accuracy of 0.275\n\n\n\n\n\n    <matplotlib.legend.Legend at 0x7f1f9a216a10>\n\n![png](./qml_102_12_2.png)\n\n## Training\n\nWe once again have to train the circuit. However, this time it does not have a single training variable, but four. We therefore have to fall back to [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/optimize.html) package to optimize the target function.\n\n```python\nfrom scipy.optimize import minimize\n```\n\n```python\ndef get_cost_for_circ(xvals, yvals, machine=sim):\n    \"\"\"\n    Runs parametrized circuit\n\n    Args:\n        x: position of the dot\n        y: its state label\n        params: parameters of the circuit\n    \"\"\"\n\n    def execute_circ(params_flat):\n        weight = params_flat[0]\n        alpha = params_flat[1]\n        accuracy, y_pred = get_accuracy(qc, alpha=alpha, weight=weight, xvals=xvals, yvals=yvals)\n        print(f\"accuracy = {accuracy}\")\n        return 1-accuracy\n\n    return execute_circ\n```\n\n```python\ntotal_cost = get_cost_for_circ(x_train, y_train, sim)\n\n# initial parameters which are randomly initialized\nnp.random.seed(123)\nparams = np.random.uniform(size=2)\nparams_flat = params.flatten()\n\n# minimze with COBYLA optimize, which often performs quite well\nres = minimize(total_cost, params_flat, method=\"COBYLA\")\n```\n\n    accuracy = 0.675\n    accuracy = 0.6\n    accuracy = 0.775\n    accuracy = 0.65\n    accuracy = 0.4\n    accuracy = 0.9\n    accuracy = 0.65\n    accuracy = 0.65\n    accuracy = 0.825\n    accuracy = 0.825\n    accuracy = 1.0\n    accuracy = 0.925\n    accuracy = 0.9625\n    accuracy = 1.0\n    accuracy = 0.9625\n    accuracy = 0.975\n    accuracy = 1.0\n    accuracy = 1.0\n    accuracy = 1.0\n    accuracy = 1.0\n    accuracy = 1.0\n    accuracy = 1.0\n\nWe can see that the accuracy is converging throughout the training quite nicely and it is now time to look into the optimal training parameters.\n\n```python\nopt_weight, opt_alpha = res.x\nprint(f\"optimal weight = {opt_weight}\")\nprint(f\"optimal alpha = {opt_alpha}\")\n```\n\n    optimal weight = 0.5093757145525236\n    optimal alpha = 1.3032510317367447\n\nWe can now test the accuracy on the optimal value of the weights again to test the accuracy.\n\n```python\naccuracy, y_pred = get_accuracy(qc, weight=opt_weight, alpha = opt_alpha, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nf, ax = plt.subplots()\nax.plot(x_train, y_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n\n\nprint(f\"The trained circuit has an accuracy of {accuracy:.2}\")\n```\n\n    The trained circuit has an accuracy of 1.0\n\n![png](./qml_102_20_1.png)\n\n## Test\n\nHaving finished the training, we can test the circuit now on data points that it has never seen.\n\n```python\ntest_accuracy, y_test_pred = get_accuracy(\n    qc, weight=opt_weight, alpha = opt_alpha, xvals=x_test, yvals=y_test\n)\n\nfalse_label = abs(y_test_pred - y_test) > 0\n\nx_false = x_test[false_label]\ny_false = y_test_pred[false_label]\n\nprint(f\"The circuit has a test accuracy of {test_accuracy:.2}\")\nf, ax = plt.subplots()\nax.plot(x_test, y_test_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n```\n\n    The circuit has a test accuracy of 1.0\n\n\n\n\n\n    <matplotlib.legend.Legend at 0x7f1f99613150>\n\n![png](./qml_102_22_2.png)\n\n## Summary and outlook\n\nIn this tutorial, we have seen that the extension of the circuit to deeper structures allows us to learn training sets, which were previously impossible to evaluate. In the next tutorials, we will extend these circuits in two directions:\n\n1. Work with larger dimensions of input parameters. This is what we would like to do quantum machine learning anyways and where the main innovation of the data reuploading circuits lies.\n2. Work on circuits with multiple qubits to classify different classes and introduce entanglement in a systematic fashion.",
    "order": 2
  },
  {
    "title": "QML 103 - Teaching the circle",
    "content": "\nIn the [last tutorial](./2) we saw how deeper algorithms allow us to model more complex dependencies of single input variables and binary labels. However, it is now time to train the algorithms on problems with more than a single input variable it is common in image recognition etc.\n\nIn this tutorial, we will introduce slightly more complex data sets and see how we can handle the classification with data reuploading circuits. We will learn:\n\n- that more input parameters require deeper circuits.\n- the required dimension of the circuit is only set by the number of labels in the problem. So in this most basic ideas of quantum machine learning algorithms.\n\nWe will always focus on simplicity throughout this tutorial and leave the more complex discussions to the extensive literature and later tutorials.\n\n```python\n# only necessary on colab to have all the required packages installed\n\n!pip install qiskit\n!pip install pylatexenc\n```\n\nAnd we also install the other dependencies\n\n```python\nfrom typing import Union, List\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\n# for splitting the data set\nfrom sklearn.model_selection import train_test_split\n\n# for the quantum circuits\nfrom qiskit.circuit import QuantumCircuit, Parameter\nfrom qiskit import Aer\n```\n\n## Learning two-dimensional inputs\n\nWe will now work with a problem with data that have two-dimensional input $\\mathbf{x}_i = (x_{1,i}, x_{2,i})$ and as previously a binary label $y_i \\in \\{0, 1\\}$. A nice example of such a data-set is a circle in a plane as visualized below.\n\n```python\ndef circle(samples: int, center: List[float] = [0.0, 0.0], radius:float =np.sqrt(2 / np.pi)) -> Union[np.array, np.array]:\n    \"\"\"\n    Produce the data set of a circle in a plane that spans from -1 to +1\n\n    Args:\n      sample: The number of samples we would like to generate\n      center: where should be the center of the circle\n      radius: What should be the radius of the sample\n    Returns:\n      The array of the xvalues and the array of the labels.\n    \"\"\"\n    xvals, yvals = [], []\n\n    for i in range(samples):\n        x = 2 * (np.random.rand(2)) - 1\n        y = 1\n        if np.linalg.norm(x) < radius:\n            y = 0\n        xvals.append(x)\n        yvals.append(y)\n    return np.array(xvals), np.array(yvals)\n\n\ndef plot_data(x, y, fig=None, ax=None, title=None):\n\n    if fig == None:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    reds = y == 0\n    blues = y == 1\n    ax.scatter(x[reds, 0], x[reds, 1], c=\"red\", s=20, edgecolor=\"k\")\n    ax.scatter(x[blues, 0], x[blues, 1], c=\"blue\", s=20, edgecolor=\"k\")\n    ax.set_xlabel(\"$x_1$\")\n    ax.set_ylabel(\"$x_2$\")\n    ax.set_title(title)\n\n\nxdata, ydata = circle(200)\nfig, ax = plt.subplots(1, 1, figsize=(4, 4))\nplot_data(xdata, ydata, fig=fig, ax=ax)\n```\n\n![png](./qml_103_5_0.png)\n\nOnce again we split the data set and get into the training.\n\n```python\nx_train, x_test, y_train, y_test = train_test_split(\n    xdata, ydata, test_size=0.20, random_state=42\n)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n\nplot_data(x_train, y_train, fig=f, ax=ax1, title=\"training data\")\nplot_data(x_test, y_test, fig=f, ax=ax2, title=\"test data\")\n\nf.tight_layout()\n```\n\n![png](./qml_103_7_0.png)\n\n## Working with multiple inputs\n\nTo achieve training now, we have to upload all the input parameters into the circuit. We will once again follow the [data-reuploading approach](https://quantum-journal.org/papers/q-2020-02-06-226/). In summary, we will\n\n1. Prepare the initial state.\n2. Apply a parametrized circuit with parameters $\\mathbf{w}$ that depend on the input $U(\\mathbf{w}, \\mathbf{x}_i)$.\n3. Read out the label from the measurement of the qubit.\n\nThe main difference is now that the upload $S(\\mathbf{x}_i)$ will have multiple rotations. In our fairly simple case of two input parameters, we will then set it as:\n\n$$\n\\hat{S}(x_{1,i}, x_{2,i}) = \\hat{R}_z(w_2 \\cdot x_2) \\hat{R}_x(w_1 \\cdot x_1)\n$$\n\nLet us just visualize it once in `qiskit`.\n\n```python\nsim = Aer.get_backend(\"aer_simulator\")\n```\n\n```python\ntheta1 = Parameter(r\"$\\theta_1$\")\ntheta2 = Parameter(r\"$\\theta_2$\")\n\nalpha0 = Parameter(r\"$\\alpha_0$\")\nalpha1 = Parameter(r\"$\\alpha_1$\")\n\nbeta0 = Parameter(r\"$\\beta_0$\")\nbeta1 = Parameter(r\"$\\beta_1$\")\n\ngamma0 = Parameter(r\"$\\gamma_0$\")\ngamma1 = Parameter(r\"$\\gamma_1$\")\n\n\n#alpha1 = Parameter(r\"$\\alpha_1$\")\nqc = QuantumCircuit(1)\n\n# first upload\nqc.rx(theta1, 0)\nqc.rz(theta2, 0)\n\n# first processing\nqc.rx(alpha0, 0)\nqc.rz(beta0, 0)\nqc.rx(gamma0, 0)\n\n# second upload\nqc.rx(theta1, 0)\nqc.rz(theta2, 0)\n\n# second processing\nqc.rx(alpha1, 0)\nqc.rz(beta1, 0)\nqc.rx(gamma1, 0)\n\n\nqc.measure_all()\nqc.draw(\"mpl\")\n```\n\n![png](./qml_103_10_0.png)\n\nWe can now look at the performance of the code with some randomly initialized weight in predicting the appropiate label.\n\n```python\ndef get_accuracy(\n    qc: QuantumCircuit, weights: List[float] , alphas: List[float], betas: List[float], gammas: List[float],\n    xvals: List[float], yvals: List[int]\n) -> Union[float, List[int]]:\n    \"\"\"\n    Calculates the accuracy of the circuit for a given set of data.\n\n    Args:\n      qc: the quantum circuit\n      alphas: the training parameters for the z processing gate\n      gammas: the training parameters for the x processing gate\n      weights: the weights for the inputs\n      xvals: the input values\n      yvals: the labels\n    Returns:\n      The accuracy and the predicted labels.\n    \"\"\"\n    pred_labels = np.zeros(len(xvals))\n    accurate_prediction = 0\n    for ii, xinput, yinput in zip(range(len(xvals)), xvals, yvals.astype(int)):\n        # set the circuit parameter\n        circuit = qc.assign_parameters(\n            {theta1: weights[0]*xinput[0],\n             theta2: weights[1]*xinput[1],\n             alpha0: alphas[0],\n             alpha1: alphas[1],\n             beta0: betas[0],\n             beta1: betas[1],\n             gamma0: gammas[0],\n             gamma1: gammas[1],\n             },\n            inplace=False,\n        )\n        # run the job and obtain the counts\n        Nshots = 4000\n        job = sim.run(circuit, shots=Nshots)\n        counts1 = job.result().get_counts()\n\n        # obtain the predicted label on average\n        if \"0\" in counts1:\n            pred_label = 1 * (counts1[\"0\"] < Nshots/2)\n        else:\n            pred_label = 1\n        pred_labels[ii] = pred_label\n        if yinput == pred_label:\n            accurate_prediction += 1\n    return accurate_prediction / len(yvals), pred_labels\n```\n\n```python\nnp.random.seed(123)\n\nweights = np.random.uniform(size=2)\nalphas = np.random.uniform(size=2)\nbetas = np.random.uniform(size=2)\ngammas = np.random.uniform(size=2)\n\naccuracy, y_pred = get_accuracy(qc, alphas=alphas, betas = betas, weights=weights, gammas = gammas, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nprint(f\"The randomly initialized circuit has an accuracy of {accuracy}\")\n```\n\n    The randomly initialized circuit has an accuracy of 0.3875\n\nNow it is time to visualize the quality of the prediction with random parameters. We can clearly see that the overlap is ... random.\n\n```python\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n\nplot_data(x_train, y_train, fig=f, ax=ax1, title=\"training data\")\nplot_data(x_false, y_false, fig=f, ax=ax2, title=\"false label\")\n```\n\n![png](./qml_103_15_0.png)\n\n## Training\n\nWe once again have to train the circuit as discussed in the[ previous tutorial](qml102) with [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/optimize.html) package to optimize the target function.\n\n```python\nfrom scipy.optimize import minimize\n```\n\n```python\ndef get_cost_for_circ(xvals, yvals, machine=sim):\n    \"\"\"\n    Runs parametrized circuit\n\n    Args:\n        x: position of the dot\n        y: its state label\n        params: parameters of the circuit\n    \"\"\"\n\n    def execute_circ(params_flat):\n        weights = params_flat[:2]\n        alphas = params_flat[2:4]\n        betas = params_flat[4:6]\n        gammas = params_flat[6:]\n        accuracy, y_pred = get_accuracy(qc, alphas=alphas, betas = betas, weights=weights, gammas = gammas, xvals=xvals, yvals=yvals)\n        print(f\"accuracy = {accuracy}\")\n        return 1-accuracy\n\n    return execute_circ\n```\n\n```python\ntotal_cost = get_cost_for_circ(x_train, y_train, sim)\n\n# initial parameters which are randomly initialized\nnp.random.seed(123)\nparams = np.random.uniform(size=8)\nparams_flat = params.flatten()\n\n# initial parameters which are guessed\nweights = np.array([1,1])\nalphas = [0,0]\nbetas = [0,0]\ngammas = [0,0]\nparams_flat = np.zeros(8)\n#params_flat[:2] = 1\n\n# minimze with COBYLA optimize, which often performs quite well\nres = minimize(total_cost, params_flat, method=\"COBYLA\")\n```\n\n    accuracy = 0.53125\n    accuracy = 0.70625\n    accuracy = 0.6875\n    accuracy = 0.5875\n    accuracy = 0.5875\n    accuracy = 0.5875\n    accuracy = 0.70625\n    accuracy = 0.5875\n    accuracy = 0.5875\n    accuracy = 0.5\n    accuracy = 0.6\n    accuracy = 0.55625\n    accuracy = 0.70625\n    accuracy = 0.725\n    accuracy = 0.71875\n    accuracy = 0.71875\n    accuracy = 0.725\n    accuracy = 0.69375\n    accuracy = 0.725\n    accuracy = 0.73125\n    accuracy = 0.7125\n    accuracy = 0.73125\n    accuracy = 0.71875\n    accuracy = 0.71875\n    accuracy = 0.74375\n    accuracy = 0.73125\n    accuracy = 0.7375\n    accuracy = 0.71875\n    accuracy = 0.73125\n    accuracy = 0.74375\n    accuracy = 0.70625\n    accuracy = 0.74375\n    accuracy = 0.7125\n    accuracy = 0.74375\n    accuracy = 0.725\n    accuracy = 0.725\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.73125\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.7375\n    accuracy = 0.74375\n    accuracy = 0.73125\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.7375\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.7375\n    accuracy = 0.74375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.73125\n    accuracy = 0.74375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.74375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.74375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.7375\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.74375\n    accuracy = 0.7375\n\nWe can see that the accuracy is converging to a value of roughly slightly more than 70% nd it is now time to look into the optimal training parameters.\n\n```python\nopt_weights = res.x[:2]\nopt_alphas = res.x[2:4]\nopt_betas = res.x[4:6]\nopt_gammas = res.x[6:8]\n\nprint(f\"optimal weights = {opt_weights}\")\nprint(f\"optimal alpha = {opt_alphas}\")\nprint(f\"optimal betas = {opt_betas}\")\nprint(f\"optimal gammas = {opt_gammas}\")\n```\n\n    optimal weights = [1.17532389 0.86949734]\n    optimal alpha = [ 0.40087815 -0.07970246]\n    optimal betas = [-0.03898444  1.24761263]\n    optimal gammas = [-0.31645471 -0.01597381]\n\nWe can now test the accuracy on the optimal value of the weights again to test the accuracy.\n\n```python\naccuracy, y_pred = get_accuracy(qc, weights=opt_weights, alphas = opt_alphas, betas = opt_betas, gammas = opt_gammas, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nprint(f\"The trained circuit has an accuracy of {accuracy:.2}\")\n```\n\n    The trained circuit has an accuracy of 0.73\n\n```python\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n\nplot_data(x_train, y_train, fig=f, ax=ax1, title=\"training data\")\nplot_data(x_false, y_false, fig=f, ax=ax2, title=\"false label\")\n```\n\n![png](./qml_103_24_0.png)\n\n## Test\n\nHaving finished the training, we can test the circuit now on data points that it has never seen.\n\n```python\ntest_accuracy, y_test_pred = get_accuracy(\n    qc, weights=opt_weights,  alphas = opt_alphas, betas = opt_betas, gammas = opt_gammas, xvals=x_test, yvals=y_test\n)\n\nfalse_label = abs(y_test_pred - y_test) > 0\n\nx_false = x_test[false_label]\ny_false = y_test_pred[false_label]\n\nprint(f\"The circuit has a test accuracy of {test_accuracy:.2}\")\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n\nplot_data(x_train, y_train, fig=f, ax=ax1, title=\"training data\")\nplot_data(x_false, y_false, fig=f, ax=ax2, title=\"false label\")\n```\n\n    The circuit has a test accuracy of 0.78\n\n![png](./qml_103_26_1.png)\n\n## Summary and outlook\n\nIn this tutorial, we have seen that the data reuploading circuit can be extended to handle multiple input parameters. So have put everything together that we would like at this stage:\n\n1. More input parameters can be handle through more gates in the layer $S(\\mathbf{x}_i)$\n2. More complex data structures can be embedded through deeper circuit structures.\n\nImproved circuits typically work with tuning of:\n\n- Different optimizers.\n- Cost functions that are easier to differentiate.\n- Libraries that perform faster differentiation.\n- More parameters for the circuits.\n\nUp to this point we have exclusively worked with circuits that work on a single qubit. While this allowed us to learn more fundamental concepts, it clearly ignores one important parameter that is commonly seen as a differentiator between classical and quantum systems, namely entanglement. So in the next and last tutorial of this series we will work with multiple qubits, entangle them and see how we can systematically study the role of entanglement in the circuit performance.",
    "order": 3
  },
  {
    "title": "QML 104 - More is different ? Working with multiple qubits",
    "content": "\nIn the [previous tutorial](./3) we saw how we might be able to handle more complex inputs through reuploading. However, we only discussed algorithms with single qubits until now and there is little chance that such algorithms have an impact beyond their pedagogical power.\n\nIn this tutorial, we will present one for to introduce multiple qubits and entangle them. We will learn:\n\n- How multiple qubits can work with multiple labels.\n- Entanglement might be used to control the performance of the circuit.\n\nWe will always focus on simplicity throughout this tutorial and leave the more complex discussions to the extensive literature.\n\n```python\nfrom typing import Union, List\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\n# for splitting the data set\nfrom sklearn.model_selection import train_test_split\n\n# for the quantum circuits\nfrom qiskit.circuit import QuantumCircuit, Parameter\nfrom qiskit import Aer\n```\n\n## A multi-label data set\n\nIn the [first tutorials](./1) we saw how to label data that depend only on one input and had binary labels $y_i \\in \\{0, 1\\}$. In the [third tutorial](./3), we worked with a data set that had a two-dimensional input $\\mathbf{x}_i = (x_{1,i}, x_{2,i})$ and a binary label $y_i \\in \\{0, 1\\}$. We will now build up on this work and use a data set that has:\n\n- one dimensional input $x$.\n- Labels that go from $y_i \\in \\{0, 1, 2, 3\\}$. Extending our previous data set.\n\n```python\nnp.random.seed(1)\nxdata = np.random.uniform(0, 2, 100)\nydata = np.ceil(2*xdata)-1\n\nf, ax = plt.subplots()\nax.plot(xdata, ydata, \"o\")\nax.set_xlabel(\"input value\")\nax.set_ylabel(\"label\");\n```\n\n![png](./qml_104_4_0.png)\n\nOnce again we split the data set and get into the training.\n\n```python\nx_train, x_test, y_train, y_test = train_test_split(\n    xdata, ydata, test_size=0.20, random_state=42\n)\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\nax1.plot(x_train, y_train, \"o\")\nax1.set_xlabel(\"input value\")\nax1.set_ylabel(\" given labels\")\nax1.set_title(\"training data\")\n\nax2.plot(x_test, y_test, \"o\")\nax2.set_xlabel(\"input value\")\nax2.set_title(\"test data\")\n\nf.tight_layout()\n```\n\n![png](./qml_104_6_0.png)\n\n## Handling multiple labels\n\nTo achieve training now, we have to handle labels that go beyond binary. Quite interestingly there is a multitude of approaches on how to encode such labels, one of them was propose in the original paper [data-reuploading approach](https://quantum-journal.org/papers/q-2020-02-06-226/). However, we will follow a more intuitive path in which we simply use the qubits as [binary encoding](https://en.wikipedia.org/wiki/Binary_number) of the labels. To represent the number $3$, we therefore need two qubits as the result $|11\\rangle$ is then translated in $2^1+1^1 = 3$. In summary, we will\n\n1. Prepare the initial state, where multiple qubits are initialized in $0$.\n2. Apply a parametrized circuit with parameters $\\mathbf{w}$ that depend on the input $U(\\mathbf{w},x_i)$.\n3. Read out the label from the measurement of the qubit.\n\nThe main difference is now that we have multiple qubits that we should handle in this approach and we have to choose how to properly parametrize the circuit. This falls into the large class of variational circuits, about which we might do a series of tutorials, if interest exists. A nice review on different choices can be found [in this paper](https://arxiv.org/abs/2008.08605). Here, we will choose the following parametrization.\n\n1. We apply a rotation gate $R_x$ on each qubit.\n2. We entangle the two qubit through a parametrized two qubit gate. A wide choice exists, but the $R_{zz}$ gate is especially nice as it creates entanglement and does not commute with the $R_x$ gates.\n3. We apply another rotation gate $R_x$ on each qubit.\n\nLet us just visualize it once in `qiskit`.\n\n```python\nsim = Aer.get_backend(\"aer_simulator\")\n```\n\n```python\ntheta1 = Parameter(r\"$\\theta_1$\")\ntheta2 = Parameter(r\"$\\theta_2$\")\n\nalpha0 = Parameter(r\"$\\alpha_0$\")\n\nqc = QuantumCircuit(2)\n\n# first upload\nqc.rx(theta1, 0)\nqc.rx(theta2, 1)\n\n# first processing\nqc.rzz(alpha0, 0,1)\n\n# second upload\nqc.rx(theta1, 0)\nqc.rx(theta2, 1)\n\n\nqc.measure_all()\nqc.draw(\"mpl\")\n```\n\n![png](./qml_104_9_0.png)\n\nWe can now look at the performance of the code with some randomly initialized weight in predicting the appropiate label.\n\n```python\ndef get_accuracy(\n    qc: QuantumCircuit, weights: List[float] , alphas: List[float], xvals: List[float], yvals: List[int]) -> Union[float, List[int]]:\n    \"\"\"\n    Calculates the accuracy of the circuit for a given set of data.\n\n    Args:\n      qc: the quantum circuit\n      alphas: the training parameters for the z processing gate\n      gammas: the training parameters for the x processing gate\n      weights: the weights for the inputs\n      xvals: the input values\n      yvals: the labels\n    Returns:\n      The accuracy and the predicted labels.\n    \"\"\"\n    pred_labels = np.zeros(len(xvals))\n    accurate_prediction = 0\n    for ii, xinput, yinput in zip(range(len(xvals)), xvals, yvals.astype(int)):\n        # set the circuit parameter\n        circuit = qc.assign_parameters(\n            {theta1: weights[0]*xinput,\n             theta2: weights[1]*xinput,\n             alpha0: alphas\n             },\n            inplace=False,\n        )\n        # run the job and obtain the counts\n        Nshots = 4000\n        job = sim.run(circuit, shots=Nshots)\n        counts1 = job.result().get_counts()  # e.g. counts = {\"00\": 2000, \"11\": 2000}\n\n        # obtain the predicted label on average\n        av_label = 0\n        for el in counts1:\n          av_label += int(el,2)*counts1[el]/Nshots\n        pred_label = round(av_label)\n\n        pred_labels[ii] = pred_label\n        if yinput == pred_label:\n            accurate_prediction += 1\n    return accurate_prediction / len(yvals), pred_labels\n```\n\n```python\nnp.random.seed(123)\n\nweights = np.random.uniform(size=2)\nalphas = np.random.uniform()\n\n\naccuracy, y_pred = get_accuracy(qc, alphas=alphas, weights=weights, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nprint(f\"The randomly initialized circuit has an accuracy of {accuracy}\")\n\nf, ax = plt.subplots()\nax.plot(x_train, y_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n```\n\n    The randomly initialized circuit has an accuracy of 0.275\n\n\n\n\n\n    <matplotlib.legend.Legend at 0x7f016e522a50>\n\n![png](./qml_104_12_2.png)\n\n## Training\n\nWe once again have to train the circuit as discussed in the[ previous tutorial](./3) with [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/optimize.html) package to optimize the target function.\n\n```python\nfrom scipy.optimize import minimize\n```\n\n```python\ndef get_cost_for_circ(xvals, yvals, machine=sim):\n    \"\"\"\n    Runs parametrized circuit\n\n    Args:\n        x: position of the dot\n        y: its state label\n        params: parameters of the circuit\n    \"\"\"\n\n    def execute_circ(params_flat):\n        weights = params_flat[:2]\n        alphas = params_flat[2]\n        accuracy, y_pred = get_accuracy(qc, alphas=alphas, weights=weights, xvals=xvals, yvals=yvals)\n        print(f\"accuracy = {accuracy}\")\n        return 1-accuracy\n\n    return execute_circ\n```\n\n```python\ntotal_cost = get_cost_for_circ(x_train, y_train, sim)\n\n# initial parameters which are randomly initialized\nnp.random.seed(123)\nparams = np.random.uniform(size=3)\nparams_flat = params.flatten()\n\n# params, which are guessed close to what we know to be a good result\nparams_flat = [0.9,0.9,0.7]\n\n# minimze with COBYLA optimize, which often performs quite well\nres = minimize(total_cost, params_flat, method=\"COBYLA\")\n```\n\n    accuracy = 0.975\n    accuracy = 0.6125\n    accuracy = 0.3125\n    accuracy = 0.375\n    accuracy = 0.2625\n    accuracy = 0.5375\n    accuracy = 0.8\n    accuracy = 0.8\n    accuracy = 0.75\n    accuracy = 0.9375\n    accuracy = 0.95\n    accuracy = 0.95\n    accuracy = 0.9625\n    accuracy = 0.975\n    accuracy = 0.975\n    accuracy = 0.9875\n    accuracy = 0.9625\n    accuracy = 0.95\n    accuracy = 0.975\n    accuracy = 0.9625\n    accuracy = 0.975\n    accuracy = 0.975\n    accuracy = 0.9875\n    accuracy = 0.9875\n    accuracy = 0.9625\n    accuracy = 0.95\n    accuracy = 0.9625\n    accuracy = 0.975\n    accuracy = 0.9625\n    accuracy = 0.975\n    accuracy = 0.975\n    accuracy = 0.975\n    accuracy = 0.975\n    accuracy = 0.975\n    accuracy = 0.975\n    accuracy = 0.9625\n    accuracy = 0.975\n\nWe can see that the accuracy is converging to a value of more than 95% and it is now time to look into the optimal training parameters.\n\n```python\nopt_weights = res.x[:2]\nopt_alphas = res.x[2]\n\nprint(f\"optimal weights = {opt_weights}\")\nprint(f\"optimal alpha = {opt_alphas}\")\n```\n\n    optimal weights = [0.9010178  0.90824815]\n    optimal alpha = 0.7131673355135238\n\nWe can now test the accuracy on the optimal value of the weights again to test the accuracy.\n\n```python\naccuracy, y_pred = get_accuracy(qc, weights=opt_weights, alphas = opt_alphas, xvals=x_train, yvals=y_train)\n\nfalse_label = abs(y_pred - y_train) > 0\n\nx_false = x_train[false_label]\ny_false = y_pred[false_label]\n\nprint(f\"The trained circuit has an accuracy of {accuracy:.2}\")\n\nf, ax = plt.subplots()\nax.plot(x_train, y_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n```\n\n    The trained circuit has an accuracy of 0.96\n\n\n\n\n\n    <matplotlib.legend.Legend at 0x7f016e5952d0>\n\n![png](./qml_104_20_2.png)\n\nWe can see quite excellent training for this data set. One common question that always comes up for these kinds of circuits is also about the potential role of entanglement. The simplest thing would be to simply set the alpha parameter to zero.\n\n```python\naccuracy_wo_entanglement, _ = get_accuracy(qc, weights=opt_weights, alphas = 0, xvals=x_train, yvals=y_train)\nprint(f\"The trained circuit without entanglement has an accuracy of {accuracy_wo_entanglement:.2}\")\n```\n\n    The trained circuit without entanglement has an accuracy of 0.79\n\nWe can see that the correlation between the two qubits plays and substantial role in the prediction of the labels. The training of the circuit without any kind of entanglement is left to the reader. Or you just send us a comment if you would like to have a cleaner introduction on this issue.\n\n## Test\n\nHaving finished the training, we can test the circuit now on data points that it has never seen.\n\n```python\ntest_accuracy, y_test_pred = get_accuracy(\n    qc, weights=opt_weights,  alphas = opt_alphas, xvals=x_test, yvals=y_test\n)\n\nfalse_label = abs(y_test_pred - y_test) > 0\n\nx_false = x_test[false_label]\ny_false = y_test_pred[false_label]\n\nprint(f\"The circuit has a test accuracy of {test_accuracy:.2}\")\n\nf, ax = plt.subplots()\nax.plot(x_test, y_test_pred, \"o\", label=\"predicted label\")\nax.plot(x_false, y_false, \"ro\", label=\"false label\")\nax.legend()\n```\n\n    The circuit has a test accuracy of 1.0\n\n\n\n\n\n    <matplotlib.legend.Legend at 0x7f016e403c50>\n\n![png](./qml_104_25_2.png)\n\n## Summary and outlook\n\nIn this last tutorial of this introductory series, we have seen that the data reuploading can be extended towards multiple labels.\n\n- The crucial step was the extension to multiple qubits.\n- To make the algorithm work efficiently we entangled the qudits with an entanglement gate and then trained the full circuit with the whole data set.\n\nOf course, we have not gone into complex data-sets like the MNIST or other problems in this series. This will be up to the more complex literature or more advanced courses. However, we hope that this series gave you a basic idea of some fairly common concepts that are used in the field nowadays.",
    "order": 4
  }
]
